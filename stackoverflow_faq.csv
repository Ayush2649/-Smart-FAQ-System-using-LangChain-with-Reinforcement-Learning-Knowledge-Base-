topic,question,answer
reinforcement learning,K-Arms Bandit Epsilon-Greedy Policy,"Your average reward is around 0 because it is the correct estimation. Your reward function is defined as:
 # R bandit(A)
 r = np.random.normal(0, 0.01, 1)

This means the expected value of your reward distribution is 0 with 0.01 variance. In the book the authors use a different reward function. While this still has a fundamental issue, you could earn similar rewards if you change your code to
 # R bandit(A)
 r = np.random.normal(1.25, 0.01, 1)

It makes sense to give each bandit a different reward function or all your action values will be the same. So what you really should do is sample from k different distributions with different expected values. Otherwise action selection is meaningless.
Add this to your init function:
self.expected_vals = np.random.uniform(0, 2, self.k)

and change the the calculation of the reward so, that it depends on the action:
r = np.random.uniform(self.expected_vals[a], 0.5, 1)

I've also increased the variance to 0.5 as 0.01 is basically meaningless variance in the context of bandits. If your agents works correctly, his average reward should be approximately equal to np.max(self.expected_vals)
"
reinforcement learning,`stack()` vs `cat()` in PyTorch,"stack

Concatenates sequence of tensors along a new dimension.

cat

Concatenates the given sequence of seq tensors in the given dimension.

So if A and B are of shape (3, 4):

torch.cat([A, B], dim=0) will be of shape (6, 4)
torch.stack([A, B], dim=0) will be of shape (2, 3, 4)

"
reinforcement learning,Reproducibility of JAX calculations,"On an accelerator like GPU, there will generally be a tradeoff between strict bit-wise reproducibility and speed of computation.
Why is this? Fundamentally, this is because of the fact that floating point arithmetic only approximates real arithmetic, and so the order in which operations are executed can change the results, and order of operations is a degree of freedom that the GPU can exploit to execute code faster.
As a simple example, consider summing the same array in different orders:
In [1]: import numpy as np

In [2]: rng = np.random.default_rng(0)

In [3]: x = rng.normal(size=10000).astype('float32')

In [4]: x.sum()
Out[4]: np.float32(63.11888)

In [5]: x[::-1].sum()
Out[5]: np.float32(63.118877)

The results differ slightly.
This is relevant to your question because of the way a GPU works: GPUs do fast vector operations by automatically running them in parallel. So, for example, to compute a sum, it might chunk the array across N cores, sum each chunk individually, and then accumulate the intermediate sums to get the final result.
If you only care mainly about speed, you can sacrifice reproducibility and accumulate those intermediate sums in the order they're ready, which might vary from run to run, and therefore produce slightly different results. If you care mainly about reproducibility, then you have to sacrifice some speed by ensuring that you accumulate those intermediate sums in exactly the same order every time, which may leave the process waiting for a slower chunk even if a faster chunk is already ready. This is a simplistic example but the same principal applies for any computation parallelized on a GPU.
So fundamentally speaking, there will always be a tradeoff between bitwise reproducibility and speed of computation. You've already discovered the primary flags for controlling this tradeoff (XLA_FLAGS=""--xla_gpu_deterministic_ops""  and JAX_DISABLE_MOST_FASTER_PATHS=1 ). Your question seems to be ""can I somehow get both speed and strict bitwise reproducibility at once"": the answer to that question is No.
"
reinforcement learning,Reinforcement learning target data,"I believe you are talking about scaling learning horizontally as in training multiple agents in parallel.
A3C is one algorithm that does this by training multiple agents in parallel and independently of each other. Each agent has its own environment which allows it to gain a different experience than the rest of the agents, ultimately increasing the breadth of your agents collective experience. Eventually each agent updates a shared network asynchronously and you use this network to drive your main agent.
You mentioned that you wanted to use the same environment for all parallel agents. I can think of this in two ways:

If you are talking about a shared environment among agents, then this could possibly speed things up however you are likely not going to gain much in terms of performance. You are also very likely to face issues in terms of episode completion - if multiple agents are taking steps simultaneously then your transitions will be a mess to say the least. The complexity cost is high and the benefit is negligible.

If you are talking about cloning the same environment for each agent then you end up both gaining speed and a broader experience which translates to performance. This is probably the sane thing to do.


"
reinforcement learning,How can I apply member functions of a list of objects across slices of a JAX array using vmap?,"Use jax.lax.switch to select between the functions in the list and map over the desired axis of x at the same time:
def apply_func_obj(i, x_slice):
    return jax.lax.switch(i, functions_obj, x_slice)

indices = jnp.arange(len(functions_obj)) 
# Use vmap to apply the function element-wise
results = jax.vmap(apply_func_obj, in_axes=(0, 0))(indices, x)

"
reinforcement learning,Eligibility Traces: On-line vs Off-line λ-return algorithm,"
This appears to me explained like that just for clarification and you can calculate them only for the final horizon h=T at episode termination.

This is not true. The whole point of the online λ-return algorithm is that it is online: it makes updates during the episode. This is crucial in the control setting, when actions selected are determined by the current value estimates. Even in the prediction setting, the weight updates made for earlier horizons have an effect.
This is because the final weight vector from the last horizon is always used in the calculation of the update target, the truncated lambda return. So w_1^1 is used to calculate all targets for h=2, and w_2^2 is used to calculate all targets for h=3. Because the targets are calculated using the latest weight vectors, they are generally more accurate.
Even in the prediction setting, the online lambda return algorithm outperforms the offline version because the targets it uses are better.
"
reinforcement learning,Access Hydra configuration parameters from different files,"The clean way to do it is to pass the configuration object or a sub node (or actual parameters) directly to your modules instead of depending on globals.
There are many disadvantages to storing the config in a global:

It makes testing harder
It makes running your code with more than one set of parameters in the same run harder

There is plenty of good information online about why this is bad (e.g. this).
"
reinforcement learning,What is the difference between Q-learning and SARSA?,"Yes, this is the only difference. On-policy SARSA learns action values relative to the policy it follows, while off-policy Q-Learning does it relative to the greedy policy. Under some common conditions, they both converge to the real value function, but at different rates. Q-Learning tends to converge a little slower, but has the capabilitiy to continue learning while changing policies. Also, Q-Learning is not guaranteed to converge when combined with linear approximation.
In practical terms, under the ε-greedy policy, Q-Learning computes the difference between Q(s,a) and the maximum action value, while SARSA computes the difference between Q(s,a) and the weighted sum of the average action value and the maximum: 
Q-Learning: Q(st+1,at+1) = maxaQ(st+1,a)
SARSA:      Q(st+1,at+1) = ε·meanaQ(st+1,a) + (1-ε)·maxaQ(st+1,a)
"
reinforcement learning,"what does these print(check_output([&quot;ls&quot;, &quot;../input&quot;]).decode(&quot;utf8&quot;)) mean?","Calling check_output does the command specified, ls ../input, which lists the folder input up a directory.
Then it decodes the command result for it which is in utf-8.
And then it prints it for you to see it.
"
reinforcement learning,Why is a target network required?,"
So, in summary a target network required because the network keeps changing at each timestep and the “target values” are being updated at each timestep?

The difference between Q-learning and DQN is that you have replaced an exact value function with a function approximator.  With Q-learning you are updating exactly one state/action value at each timestep, whereas with DQN you are updating many, which you understand.  The problem this causes is that you can affect the action values for the very next state you will be in instead of guaranteeing them to be stable as they are in Q-learning.
This happens basically all the time with DQN when using a standard deep network (bunch of layers of the same size fully connected).  The effect you typically see with this is referred to as ""catastrophic forgetting"" and it can be quite spectacular.  If you are doing something like moon lander with this sort of network (the simple one, not the pixel one) and track the rolling average score over the last 100 games or so, you will likely see a nice curve up in score, then all of a sudden it completely craps out starts making awful decisions again even as your alpha gets small.  This cycle will continue endlessly regardless of how long you let it run.
Using a stable target network as your error measure is one way of combating this effect.  Conceptually it's like saying, ""I have an idea of how to play this well, I'm going to try it out for a bit until I find something better"" as opposed to saying ""I'm going to retrain myself how to play this entire game after every move"".  By giving your network more time to consider many actions that have taken place recently instead of updating all the time, it hopefully finds a more robust model before you start using it to make actions.

On a side note, DQN is essentially obsolete at this point, but the themes from that paper were the fuse leading up to the RL explosion of the last few years.
"
reinforcement learning,Serialization in JAX,"Both Jax and Numpy support the Python buffer protocol, allowing for zero-copy data sharing between the different types. That being said when using jnp.array or np.array, the default is to copy (see linked docs). So right now you do an unnecessary copy of the data. So I would suggest to use np.asarray instead (and the Jax equivalent), which only copies when needed. So your code would look like:
def save_jax(path, x: jnp.array):
    y = np.asarray(x)
    np.save(path, y)

def load_jax(path):
    with open(path, ""br"") as f:
        x = np.load(f)
    y = jnp.asarray(x)
    return y

Aside from the copy the native Numpy format might not be the best choice of format in neither I/O performance nor associated meta data. For a lightweight alternative you might want to look for example into safetensors.
I hope this helps!
"
reinforcement learning,Reinforcement Learning Gymnasium ValueError,"Instead of doing:
observation = env.reset()

you should do:
observation, _ = env.reset()

Indeed, env.reset() returns a tuple (observation, info) with observation of the initial state and an auxiliary information dict. See https://gymnasium.farama.org/api/env/#gymnasium.Env.reset.
"
reinforcement learning,Action masking for continuous action space in reinforcement learning,"I think you are on the right track. I've looked into masked actions and found two possible approaches: give a negative reward when trying to take an invalid action (without letting the environment evolve), or dive deeper into the neural network code and let the neural network output only valid actions.
I've always considered this last approach as the most efficient, and your approach of introducing boundaries seems very similar to it. So as long as this is the type of mask (boundaries) you are looking for, I think you are good to go.
"
reinforcement learning,How do I get Rllib to use the GPU on my MacBook Pro?,"It seems to be an open issue on Ray's repository. From this response's date, CPU support is ok, but GPU's aren't a thing there yet.
"
reinforcement learning,Deep reinforcement learning - how to deal with boundaries in action space,"
I would say this should work (but even better than guessing is trying
  it). Other questions would be: What is the state your agent is able to observe? Are you doing reward clipping?

On the other Hand, if your agent did not learn to avoid running into walls there might be another Problem within your learning Routine (maybe there is a bug in the reward function?)
Hard coded clipping Actions might lead to a behavior which you want to see, but it certainly cuts down the Overall performance of your agent. 
Whatelse did you implement? If not done yet, it might be good to take experience replay into account.
"
reinforcement learning,Good implementations of reinforcement learning?,"Take a look at the 2009 RL-competition. One of the problem domains is a tetris game. There was a tetris problem the year before too. Here’s the 52-page final report from that year’s fifth-place finalist, which goes into a lot of detail about how the agent worked.
"
reinforcement learning,Understanding policy and value functions reinforcement learning,"
You have a policy, which is effectively a probability distribution of actions for all my states.

Yes

A value function determines the best course of actions to achieve highest reward.

No. A value function tells you, for a given policy, what the expected cumulative reward of taking action a in state s is.
Forget about value iteration and policy iteration for a moment. The two things you should try to understand are policy evaluation and policy improvement.

In policy evaluation, you figure out the state-value function for a given policy (which tells you your expected cumulative reward for being in a state and then acting according to the policy thereafter). For every state, you look at all the neighboring states and calculate the expected value of the policy in that state (weighted sum of neighbors' values by policy probabilities). You have to loop through all the states doing this over and over. This converges in the limit to the true state-value function for that policy (in practice, you stop when the changes become small).
In policy improvement, you examine a state-value function and ask, in every state, what's the best action I could take according to the value function? The action the current policy takes might not lead to the highest value neighbor. If it doesn't, we can trivially make a better policy by acting in a way to reach a better neighbor. The new policy that results is better (or at worst, the same).

Policy iteration is just repeated policy evaluation and policy improvement.
In value iteration, you truncate the evaluation step. So instead of following the full evaluation process to convergence, you do one step of looking at neighboring states, and instead of taking an expectation under the policy, you do policy improvement immediately by storing the maximum neighboring value. Evaluation and improvement are smudged together. You repeat this smudged step over and over, until the change in the values is very small. The principal idea of why this converges is the same; you're evaluating the policy, then improving it until it can no longer be improved.
There are a bunch of ways that you might go about understanding policy and value iteration. You can read more about this evaluation and improvement framing in Reinforcement Learning: An Introduction 2nd ed.. I've left out some important details about discounting, but hopefully the overall picture is clearer now.
"
reinforcement learning,Best practices for exploration/exploitation in Reinforcement Learning,"well, for that I guess it is better to use the linear annealed epsilon-greedy policy which updates epsilon based on steps:

EXPLORE = 3000000   #how many time steps to play
FINAL_EPSILON = 0.001 # final value of epsilon
INITIAL_EPSILON = 1.0# # starting value of epsilon

if epsilon > FINAL_EPSILON:
            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE



"
reinforcement learning,Reinforcement learning toy project,"If this is your first experiment with reinforcement learning I would recommend starting with something much simpler than this.  You can start simple to get the hang of things and then move to a more complicated project like this one.  I have trouble with POMDPs and I have been working in RL for quite a while now.  Now I'll try to answer what questions I can.
I think it is POMDP , but can I model it as MDP and just ignore noise?
Yes.  POMDP stands for Partially Observable Markov Decision Process.  The partially observable part refers to the fact that the agent can't know it's state perfectly, but can estimate it based on observations.  In your case, you would have the location of the rocket as an observation that can have some noise, and based on the agents previous knowledge you can update it's belief of where the missiles are.  That adds a lot of complexity.  It would be much easier to use the missile locations as absolutes and not have to deal with uncertainty.  Then you would not have to use POMDPs.
In case POMDP, What is the recommended way for evaluating probability?
I don't understand your question.  You would use some form of Bayes rule.  That is, you would have some sort of distribution that is your belief state (probabilities of being in any given state), that would be your prior distribution and based on observation you would adjust this and get a posterior distribution.  Look into Bayes rule if you need more info.
Which is better to use in this case: Value functions or Policy Iterations?
Most of my experience has been using value functions and find them relatively easy to use/understand.  But I don't know what else to tell you.  I think this is probably your choice, I would have to spend time working on the project to make a better choice.
Can I use NN to model environment dynamics instead of using explicit equations?  If yes, Is there a specific type/model of NN to be recommended?
I don't know anything about using NN to model environments, sorry.
I think Actions must be discretized, right?
Yes.  You would have to have a discrete list of actions, and a discrete list of states.  Generally the algorithm will choose the best action for any given state, and for the simplest algorithms (something like QLearning) you just keep track of a value for every given state-action pair.
If you are just learning all of this stuff I would recommend the Sutton and Barto text.  Also if you want to see a simple example of a RL algorithm I have a very simple base class and an example using it up at github (written in Python).  The abstract_rl class is meant to be extended for RL tasks, but is very simple.  simple_rl.py is an example of a simple task (it is a simple grid with one position being the goal and it uses QLearning as the algorithm) using base_rl that can be run and will print some graphs showing reward over time.  Neither are very complex, but if you are just getting started may help to give you some ideas.  I hope this helped.  Let me know if you have any more or more specific questions.
"
reinforcement learning,Reinforcement Learning,"There is no 
int state=0;
int state=x1;

in matlab. That is C style. In matlab int is a built-in function meaning something else.
Besides, it should be
q=zeros(size(R));
q1=ones(size(R))*inf;

Remember to download his RandomPermutation function, otherwise just use randperm instead.
"
reinforcement learning,Reinforcement learning And POMDP,"The problem is that the sum over all possible following states has to equal 1. If you construct your network like that, that is not guaranteed. Two possible alternatives come to my mind, where I assume discrete states.

When making a prediction, run the network for each possible following state. Afterwards, normalize by dividing through the sum of all probabilities.
Use one output per possible following state. You can then use a softmax layer (as in classification) and interpret the values which then range from 0 to 1 and sum up to 1 as probabilities.

These two are actually roughly equivalent from a mathematical perspective.
In the case of continuous variables, you will have to assume distributions (e.g. a multivariate Gaussian) and use the parameters of that distribution (e.g. mean and covariance stdev) as outputs. 
"
reinforcement learning,Reinforcement learning with neural networks,"What do you want to learn? What should be the output?
Is the input just the used action? If you are learning a model of the environment, it is expressed by a probability distribution: 
P(next_state|state, action)
It is common to use a separate model for each action.
That makes the mapping between input and output simpler.
The input is a vector of state features. The output is a vector of the features of the next state. The used action is implied by the model.
The state features could be encoded as bits. An active bit would indicate the presence of a feature.
This would learn a deterministic model. I don't know what is a good way to learn a stochastic model of the next states. One possibility may be to use stochastic neurons.
"
reinforcement learning,"Reinforcement learning algorithms for continuous states, discrete actions","Applying Q-learning in continuous (states and/or actions) spaces is not a trivial task. This is especially true when trying to combine Q-learning with a global function approximator such as a NN (I understand that you refer to the common multilayer perceptron and the backpropagation algorithm). You can read more in the Rich Sutton's page. A better (or at least more easy) solution is to use local approximators such as for example Radial Basis Function networks (there is a good explanation of why in Section 4.1 of this paper). 
On the other hand, the dimensionality of your state space maybe is too high to use local approximators. Thus, my recommendation is to use other algorithms instead of Q-learning. A very competitive algorithm for continuous states and discrete actions is Fitted Q Iteration, which usually is combined with tree methods to approximate the Q-function.
Finally, a common practice when the number of actions is low, as in your case, it is to use an independent approximator for each action, i.e., instead of a unique approximator that takes as input the state-action pair and return a Q value, using three approximators, one per action, that take as input only the state. You can find an example of this in Example 3.1 of the book Reinforcement Learning and Dynamic Programming Using Function Approximators
"
reinforcement learning,Supervised learning v.s. offline (batch) reinforcement learning,"
I am more curious about the offline (batch) setting for reinforcement learning where the dataset (collected learning experiences) is given a priori. What are the differences compared to supervised learning then ? and what are the similarities they may share ?

In the online setting, the fundamental difference between supervised learning and reinforcement learning is the need for exploration and the trade-off between exploration/exploitation in RL. However also in the offline setting there are several differences which makes RL a more difficult/rich problem than supervised learning. A few differences I can think of on the top of my head:

In reinforcement learning the agent receives what is termed ""evaluative feedback"" in terms of a scalar reward, which gives the agent some feedback of the quality of the action that was taken but it does not tell the agent if this action is the optimal action or not. Contrast this with supervised learning where the agent receives what is termed ""instructive feedback"": for each prediction that the learner makes, it receives a feedback (a label) that says what the optimal action/prediction was. The differences between instructive and evaluative feedback is detailed in Rich Sutton's book in the first chapters. Essentially reinforcement learning is optimization with sparse labels, for some actions you may not get any feedback at all, and in other cases the feedback may be delayed, which creates the credit-assignment problem.

In reinforcement learning you have a temporal aspect where the goal is to find an optimal policy that maps states to actions over some horizon (number of time-steps). If the horizon T=1, then it is just a one-off prediction problem like in supervised learning, but if T>1 then it is a sequential optimization problem where you have to find the optimal action not just in a single state but in multiple states and this is further complicated by the fact that the actions taken in one state can influence which actions should be taken in future states (i.e. it is dynamic).

In supervised learning there is a fixed i.i.d distribution from which the data points are drawn (this is the common assumption at least). In RL there is no fixed distribution, rather this distribution depends on the policy that is followed and often this distribution is not i.i.d but rather correlated.


Hence, RL is a much richer problem than supervised learning. In fact, it is possible to convert any supervised learning task into a reinforcement learning task: the loss function of the supervised task can be used as to define a reward function, with smaller losses mapping to larger rewards. Although it is not clear why one would want to do this because it converts the supervised problem into a more difficult reinforcement learning
problem. Reinforcement learning makes fewer assumptions than supervised learning and is therefore in general a harder problem to solve than supervised learning. However, the opposite is not possible, it is in general not possible to convert a reinforcement learning problem into a supervised learning problem.
"
reinforcement learning,How to train an artificial neural network to play Diablo 2 using visual input?,"I can see that you are worried about how to train the ANN, but this project hides a complexity that you might not be aware of. Object/character recognition on computer games through image processing it's a highly challenging task (not say crazy for FPS and RPG games). I don't doubt of your skills and I'm also not saying it can't be done, but you can easily spend 10x more time working on recognizing stuff than implementing the ANN itself (assuming you already have experience with digital image processing techniques).
I think your idea is very interesting and also very ambitious. At this point you might want to reconsider it. I sense that this project is something you are planning for the university, so if the focus of the work is really ANN you should probably pick another game, something more simple.
I remember that someone else came looking for tips on a different but somehow similar project not too long ago. It's worth checking it out.
On the other hand, there might be better/easier approaches for identifying objects in-game if you're accepting suggestions. But first, let's call this project for what you want it to be: a smart-bot. 
One method for implementing bots accesses the memory of the game client to find relevant information, such as the location of the character on the screen and it's health. Reading computer memory is trivial, but figuring out exactly where in memory to look for is not. Memory scanners like Cheat Engine can be very helpful for this.
Another method, which works under the game, involves manipulating rendering information. All objects of the game must be rendered to the screen. This means that the locations of all 3D objects will eventually be sent to the video card for processing. Be ready for some serious debugging.
In this answer I briefly described 2 methods to accomplish what you want through image processing. If you are interested in them you can find more about them on Exploiting Online Games (chapter 6), an excellent book on the subject.
"
reinforcement learning,Pytorch Geometric graph batching not using DataLoader for Reinforcement learning,"If I run through the batch one by one or by creating a batch, I can get the same numerical results from the neural network. I then have to reshape the output to make it fit the expected output. I find it weird that PyTorch Geometric does not do this automatically. I don't know if this is the ""correct"" way of doing it. However, this seems to be the best alternative/solution.
# setup example
batch_size = 3
num_nodes = 3
memory = np.zeros(batch_size, dtype=object)

# fill memory
for i in range(batch_size):
    memory[i] = random_pyg_graph(num_nodes=num_nodes)

# define model
CNN = DeepNetworkGCN()

# test for single PyG
for i in range(len(memory)):
    output = CNN.forward(memory[i])
    print(output)
# tensor([[-0.1082],
#         [-0.1337],
#         [-0.1323]], grad_fn=<AddmmBackward0>)
# tensor([[-0.0894],
#         [-0.0903],
#         [-0.0789]], grad_fn=<AddmmBackward0>)
# tensor([[-0.1073],
#         [-0.1131],
#         [-0.1131]], grad_fn=<AddmmBackward0>)

# Create batch and do forward pass.
output = CNN.forward(Batch.from_data_list(memory[:]))
print(output)

# tensor([[-0.1082],
#         [-0.1337],
#         [-0.1323],
#         [-0.0894],
#         [-0.0903],
#         [-0.0789],
#         [-0.1073],
#         [-0.1131],
#         [-0.1131]], grad_fn=<AddmmBackward0>)

print(output.reshape(batch_size, num_nodes))

# tensor([[-0.1082, -0.1337, -0.1323],
#         [-0.0894, -0.0903, -0.0789],
#         [-0.1073, -0.1131, -0.1131]], grad_fn=<ViewBackward0>)

"
reinforcement learning,Performance issue with gradient-bandit agent,"Solved! I am sorry, the problem was outside of the aforementioned code. Correctness of best bandit choice was coded in a wrong way!
An interesting thing:
np.choice(a_list) returns numpy.some_type variable! And when you compare this variable to another_list then numpy broadcasts this variable and compares both as array-likes!
That was something I didn't know about / pay attention to, which made the actual error in code unbeknown to me.
"
reinforcement learning,OpenAi-Gym Discrete Space with negative values,"AFAIK, in OpenAI-Gym discrete environments you have indexes for each possible action, because of that you may don't need negative values. However, you can map each action index the an arbitrary value, positive or negative.
For example, in the Cartpole environment you can apply a positive (push to the right) or a negative (push to the left) force to the cart. This problem is modeled using a discrete environment, where action 0 = negative force and action 1 = positive force. For more details check the Cartpole source code (e.g., line 95).
Similarly, in your case, although your 200 action indexes are all positive, they can represent positive or negative actions.
"
reinforcement learning,AttributeError: module &#39;_Box2D&#39; has no attribute &#39;RAND_LIMIT_swigconstant&#39;,"Try this 'pip3 install box2d box2d-kengz'
"
reinforcement learning,How can I change this to use a q table for reinforcement learning,"I have a few suggestions based on your code example:

separate the environment from the agent. The environment needs to have a method of the form new_state, reward = env.step(old_state, action). This method is saying how an action transforms your old state into a new state. It's a good idea to encode your states and actions as simple integers. I strongly recommend setting up unit tests for this method.

the agent then needs to have an equivalent method action = agent.policy(state, reward). As a first pass, you should manually code an agent that does what you think is right. e.g., it might just try to head towards the goal location.

consider the issue of whether the state representation is Markovian. If you could do better at the problem if you had a memory of all the past states you visited, then the state doesn't have the Markov property. Preferably, the state representation should be compact (the smallest set that is still Markovian).

once this structure is set-up, you can then think about actually learning a Q table. One possible method (that is easy to understand but not necessarily that efficient) is Monte Carlo with either exploring starts or epsilon-soft greedy. A good RL book should give pseudocode for either variant.


When you are feeling confident, head to openai gym https://www.gymlibrary.dev/ for some more detailed class structures. There are some hints about creating your own environments here: https://www.gymlibrary.dev/content/environment_creation/
"
reinforcement learning,Are neural networks really abandonware?,"It's true that neural networks are no longer in vogue, as they once were, but they're hardly dead. The general reason for them falling from favor was the rise of the Support Vector Machine, because they converge globally and require fewer parameter specifications.
However, SVMs are very burdensome to implement and don't naturally generalize to reinforcement learning like ANNs do (SVMs are primarily used for offline decision problems).
I'd suggest you stick to ANNs if your task seems suitable to one, as within the realm of reinforcement learning, ANNs are still at the forefront in performance.
Here's a great place to start; just check out the section titled ""Temporal Difference Learning"" as that's the standard way ANNs solve reinforcement learning problems.
One caveat though: the recent trend in machine learning is to use many diverse learning agents together via bagging or boosting. While I haven't seen this as much in reinforcement learning, I'm sure employing this strategy would still be much more powerful than an ANN alone. But unless you really need world class performance (this is what won the netflix competition), I'd steer clear of this extremely complex technique.
"
reinforcement learning,how should I improve the it/s speed ? reinforcement learning DQN,"I profiled the code manually to see what was taking the longest to compute. I measure the time taken per function using code like this. I tried import line_profiler but I couldn't get that or the alternative modules to work.
dqn_mse_loss_timer = []

start_time = time.time()
end_time = time.time()
elapsed_time = (end_time - start_time)
dqn_mse_loss_timer.append(elapsed_time)

sum_dqn_mse_loss_timer = sum(dqn_mse_loss_timer)
print(""sum_dqn_mse_loss_timer:"", sum_dqn_mse_loss_timer, ""seconds"")

"
reinforcement learning,Too many / Not enough values in OpenAI Gym Mario Model for Reinforcement Learning,"The issue is with this line : state, reward, done, info = env.step(env.action_space.sample()). you're trying to unpack env.step using 4 variables instead of 5. Take a look at the documentation of the step function here
Replace it with this :
state, reward, done, truncated , info = env.step(env.action_space.sample()

"
reinforcement learning,Understanding how to make a library recognise that I have the needed dependencies,"moviepy requires the ffmpeg command-line tool, not the ffmpeg-python Python library or the imageio_ffmpeg Python library, so looking in site-packages is useless.
shutil.which() doesn't look for libraries either, it looks for commands, so changing things there doesn't do you any good either.
You can find out if you have ffmpeg installed by typing ffmpeg on your command line.
If you have Homebrew, you can install the tool with
brew install ffmpeg

or otherwise via ffmpeg.org.
"
reinforcement learning,Understanding action &amp; observation spaces in gym for custom environments and agents,"
The majority of the problems you see are demos and simple RL applications, where state space = observation space. They are used synonymously which is not really bad habit. In very few cases at the start you find the observation space being different from the state space, however, as problems get more realistic and grounded you find the difference and the need for both.
Don't use a regular array for your action space as discrete as it might seem, stick to the gym standard, which is why it is a standard. Gym tries to standardize RL so as you progress you can simply fit your environments and problems to different RL algos.

Building new environments every time is not really ideal, it's scutwork. Sticking to the gym standard will save you tonnes of repetitive work.
"
reinforcement learning,Render() doesn&#39;t work for gym environment,"I was not able to render directly via evaluate_policy either, however here is a work around that worked for me by loading the pre-trained model and rendering the predicted the next actions:
vec_env = model.get_env() 
obs = vec_env.reset()
for i in range(1000):
    action, _states = model.predict(obs, deterministic=True) 
    obs, rewards, dones, info = vec_env.step(action) 
    vec_env.render(""human"")

"
supervised learning,How to make VScode launch.json for a Python module,"Specify the module you want to run with ""module"": ""torch.distributed.launch""
You can ignore the -m flag. Put everything else under the args key.
Note: Make sure to include --nproc_per_node and the name of file (main_swav.py) in the list of arguments
{
    ""version"": ""0.2.0"",
    ""configurations"": [
        {
            ""name"": ""Python: Current File"",
            ""type"": ""debugpy"",
            ""module"": ""torch.distributed.launch"",
            ""request"": ""launch"",
            ""console"": ""integratedTerminal"",
            ""args"": [
                ""--nproc_per_node"", ""1"", 
                ""main_swav.py"",
                ""--data_path"", ""/dataset/imagenet/train"",
            ]
        }
    ]
}

Read more here: https://code.visualstudio.com/docs/python/debugging#_module
"
supervised learning,"LaTeX, How to fit a large table in a page","As suggested by Martin Scharrer in a comment to this answer on TeX.SX, one better alternative to the command \resizebox is to use the adjustbox package. Compile the following and then compare with the same code where \begin{adjustbox}{width=\textwidth} and \end{adjustbox} are commented.
Please post a comment if you need further explainations!
\documentclass{article}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{adjustbox}

\begin{document}

\begin{table}[]
  \centering
  \caption{My caption}
  \label{my-label}

  \begin{adjustbox}{width=\textwidth}

    \begin{tabular}{lllll}
Detection Methods & Supervised /Semi-supervised/ Unsupervised & Technique Used & Applications & Technology \\
Statistical & & Gaussian-based detection & Online anomaly detection & Conventional data centres \\
Statistical & & Gaussian-based detection & General & General \\
Statistical & & Regression analysis & Globally-distributed commercial applications & Distributed, Web-based, Application \& System metrics \\
Statistical & & Regression analysis & Web applications & Enterprise web applications and conventional data centre \\
Statistical & & Correlation & Complex enterprise online applications & Distributed System \\
Statistical & & Correlation & Orleans system and distributed cloud computing services & Virtualized, cloud computing and distributed system (Orleans system) \\
Statistical & & Correlation & Hadoop, Olio and RUBiS & Virtualized cloud computing and distributed systems. \\
ĘMachine learning & Supervised & Bayesian classification & Online application & IBM system S-distributed stream processing cluster \\
Machine learning & Unsupervised & Neighbour-based technique (Local Outlier Factor algorithm) & General & Cloud Computing system \\
Machine learning & Semi-supervised & Principle component analysis and Semi-supervised Decision-tree\_ & Institute-wide cloud computing environment & Cloud Computing \\
Statistical & & Regression curve fitting the service time-adapted cumulative distributed function & Online application service & Platform and configuration agnostic \\
& & & & 
    \end{tabular}

  \end{adjustbox}

\end{table}

\end{document}


A different approach if the (too small) font size in the table is the main matter; you may want to rearrange the text in a single cell on more lines within the cell:
\documentclass{article}

\begin{document}

\begin{table}[htp]
  \centering
  \caption{My caption}
  \label{my-label}
{\small %
    \begin{tabular}{p{.18\textwidth}p{.22\textwidth}p{.2\textwidth}p{.2\textwidth}p{.2\textwidth}}
Detection\par Methods & Supervised/\par Semi-supervised/\par Unsupervised & Technique Used & Applications & Technology \\
Statistical & & Gaussian-based detection & Online anomaly detection & Conventional data centres \\
Statistical & & Gaussian-based detection & General & General \\
Statistical & & Regression\par analysis & Globally-distributed commercial applications & Distributed, Web-based, Application \&\par System metrics \\
Statistical & & Regression\par analysis & Web applications & Enterprise web applications and conventional data centre \\
Statistical & & Correlation & Complex\par enterprise online applications & Distributed\par System \\
Statistical & & Correlation & Orleans system and distributed cloud computing services & Virtualized, cloud computing and distributed system (Orleans system) \\
Statistical & & Correlation & Hadoop,\par Olio and RUBiS & Virtualized cloud computing and distributed systems. \\
ĘMachine\par learning & Supervised & Bayesian\par classification & Online\par application & IBM system S-distributed stream\par processing\par cluster \\
Machine\par learning & Unsupervised & Neighbour-based technique (Local Outlier Factor algorithm) & General & Cloud\par Computing\par system \\
Machine\par learning & Semi-supervised & Principle component analysis and Semi-supervised Decision-tree\_ & Institute-wide cloud computing environment & Cloud\par Computing \\
Statistical & & Regression curve fitting the service time-adapted cumulative distributed function & Online\par application service & Platform and configuration agnostic \\
& & & & 
    \end{tabular}%
}%
\end{table}

\end{document}

Here I used {\small ... } and \par, somewhere, to locally avoid word breaking. You should set font size first, as you prefer it, then the width of the five columns, finally locally adjust where necessary.
"
supervised learning,The easiest way for getting feature names after running SelectKBest in Scikit Learn,"You can do the following :
mask = select_k_best_classifier.get_support() #list of booleans
new_features = [] # The list of your K best features

for bool_val, feature in zip(mask, feature_names):
    if bool_val:
        new_features.append(feature)

Then change the name of your features:
dataframe = pd.DataFrame(fit_transformed_features, columns=new_features)

"
supervised learning,How do I use a registry within a supervision tree with child_spec name: {:via} syntax for other supervisor children?,"To directly answer your question, the problem is in the {:ok, _storage} = Supervisor.start_child(supervisor, {Usersystem.Storage, name: {:via, Registry, {Usersystem.Registry, ""storage""}}}).
First, there is no name in child_spec, the closest field you might find is the id, but it also does not solve your issue, since this id is local to the supervisor. Your tuple is valid enough for starting the supervisor, but the {:via, Registry, _} does not mean registering it in the registry table. You need to add this tuple to the Usersystem.Storage server instead:
defmodule Stackoverflow do
  require Logger

  def start() do
    opts = [strategy: :one_for_one, name: Usersystem.Supervisor]

    {:ok, supervisor} = Supervisor.start_link([], opts)

    # The supervisor starts a registry table, which can be used by servers later
    {:ok, _} =
      Supervisor.start_child(supervisor, {Registry, keys: :unique, name: Usersystem.Registry})

    {:ok, _storage} =
      Supervisor.start_child(
        supervisor,
        # A map child spec, check https://hexdocs.pm/elixir/1.12/Supervisor.html#child_spec/2
        %{
          # {Module callback, function callback from module, params to the function (in this case ignored)}
          start: {Usersystem.Storage, :start_link, [:ok]},
          # This id is internal to the supervisor, it is only recognizable under `Usersystem.Supervisor`
          id: :storage
        }
      )

    # I did not imported Plug.Cowboy since this example does not need it
    # {:ok, _router} = Supervisor.start_child(supervisor, {Plug.Cowboy, scheme: :http, plug: Usersystem.Router, options: [port: 8080] })

    res = Registry.lookup(Usersystem.Registry, ""storage"")

    sup_children = Supervisor.which_children(Usersystem.Supervisor)

    Logger.info(""registry response: #{inspect(res)}"")
    # [info] registry response: [{#PID<0.149.0>, nil}]

    # I will not log the long response, but note how the supervisor logs the storage child with the `:storage` id we provided
    Logger.info(""Supervisors children response: #{inspect(sup_children)}"")

    {:ok, supervisor}
  end
end

defmodule Usersystem.Storage do
  use GenServer

  def start_link(_) do
    # This will register the server properly in the Usersystem.Registry table under ""storage""
    GenServer.start_link(__MODULE__, [], name: {:via, Registry, {Usersystem.Registry, ""storage""}})
  end

  def init(_), do: {:ok, nil}
end

However, if you have only one storage server, maybe you don't even need the Registry. instead of GenServer.start_link(__MODULE__, [], name: {:via, Registry, {Usersystem.Registry, ""storage""}}), you could just do GenServer.start_link(__MODULE__, [], name: :my_storage_server. Which makes the server start under the atom name you provided. Note that you could name this as :storage and it would not conflict with the supervisor child id also called :storage at all, since the sup id is internal, I'm just using a different name to make this example clearer. You can verify the name is reachable, by simply starting the supervisor and typing: Process.where_is(:my_storage_server), which will return the id of your server. When your server restarts, it will be registered under the same atom name, so it will be available without knowing its pid. Since it is a genserver, any process calling the GenServer.call/cast passing the my_storage_server as first parameter will find the storage server.
Some notes:

Instead of adding a custom atom name to the server, we could have added the own module name: GenServer.start_link(__MODULE__, [], name: __MODULE__) or even GenServer.start_link(__MODULE__, [], name: MyCustomServerName). It is valid as long as we pass an atom not registered yet. Note that module names are just atoms under the hood.
It is a good practice to create a supervisor module for starting its children instead of creating it inline, check: https://hexdocs.pm/elixir/1.12/Supervisor.html#content
When I was starting elixir, I wish someone had recommended me the book Elixir in Action by Saša Jurić. It has clear examples of this kind of setup, including registry.

"
supervised learning,Supervised learning? or unsupervised learning? which one is correct?,"The feature maps generated by intermediate layers of a model like ResNet50 during supervised training can be considered part of the supervised learning process, though they don't directly correspond to the target labels.
During supervised learning, the optimization of parameters—including those responsible for generating feature maps—is driven by the loss function that evaluates the model’s predictions against the target labels. The feature maps are not explicitly supervised themselves (there are no direct labels for the feature maps), but their representations are indirectly shaped to improve the final classification outcome.
The intermediate layers, including conv5, learn features that are most relevant to the supervised task (image classification in this case). These features emerge as the model adjusts its weights to minimize the supervised loss, meaning the process that generates the feature maps is inherently tied to the supervised training pipeline.
In unsupervised learning, features would be extracted without reference to any labels, relying instead on intrinsic patterns in the data (e.g., clustering or autoencoders).
In supervised learning, the features are optimized to aid the ultimate supervised objective, even though the feature maps themselves are not directly compared to labels.
Since the generation of these feature maps is influenced by the supervised objective, they should be categorized as results of supervised learning.This is true even though there is no direct supervision at the level of individual feature maps, they are a byproduct of the overall supervised optimization process.
"
supervised learning,Deep learning for inferences in sequences,"
I was wondering what is the state-of-the art deep learning model to replace Hidden Markov Models (HMM)

At the moment RNN (Recurrent Neural Network) and LSTM (Long Short Term Memory) based DNNs are state of the art. They are the best for a lot of sequencing problems starting from Named Entity Recognition (https://www.quora.com/What-is-the-current-state-of-the-art-in-Named-Entity-Recognition-NER/answer/Rahul-Vadaga),  Parsing (https://arxiv.org/pdf/1701.00874.pdf) to Machine Translation (https://arxiv.org/pdf/1609.08144.pdf).
These DNNs are also called sequence models (e.g. seq2seq where input as well as output is a sequence like Machine Translation)
""unsupervised pretraining""
The pre-training is not that popular any more (for supervised ML problems) since you can achieve the same results using random restarts with parallelization as you have more (and cheaper) CPUs now.
A recent paper (Optimal Hyperparameters for Deep LSTM-Networks for Sequence Labeling Tasks
by Nils Reimers, and Iryna Gurevych) does a good comparison of various seq2seq for common NLP tasks: https://arxiv.org/pdf/1707.06799.pdf
Definitely worth a read.
"
supervised learning,Clarifying batch size when using multiple GPU&#39;s,"It's the latter of your two options. batch_size in the DataLoader specifies how many samples EACH GPU will process at once. The ""effective batch size"" is the TOTAL number of samples processed across all GPUs in one forward pass. Lightning automatically handles distributing the data:
If you set batch_size=512 with 2 GPUs:

Each GPU processes 512 samples
effective_batch_size = 512 × 2 = 1024 total samples

"
supervised learning,Supervised learning v.s. offline (batch) reinforcement learning,"
I am more curious about the offline (batch) setting for reinforcement learning where the dataset (collected learning experiences) is given a priori. What are the differences compared to supervised learning then ? and what are the similarities they may share ?

In the online setting, the fundamental difference between supervised learning and reinforcement learning is the need for exploration and the trade-off between exploration/exploitation in RL. However also in the offline setting there are several differences which makes RL a more difficult/rich problem than supervised learning. A few differences I can think of on the top of my head:

In reinforcement learning the agent receives what is termed ""evaluative feedback"" in terms of a scalar reward, which gives the agent some feedback of the quality of the action that was taken but it does not tell the agent if this action is the optimal action or not. Contrast this with supervised learning where the agent receives what is termed ""instructive feedback"": for each prediction that the learner makes, it receives a feedback (a label) that says what the optimal action/prediction was. The differences between instructive and evaluative feedback is detailed in Rich Sutton's book in the first chapters. Essentially reinforcement learning is optimization with sparse labels, for some actions you may not get any feedback at all, and in other cases the feedback may be delayed, which creates the credit-assignment problem.

In reinforcement learning you have a temporal aspect where the goal is to find an optimal policy that maps states to actions over some horizon (number of time-steps). If the horizon T=1, then it is just a one-off prediction problem like in supervised learning, but if T>1 then it is a sequential optimization problem where you have to find the optimal action not just in a single state but in multiple states and this is further complicated by the fact that the actions taken in one state can influence which actions should be taken in future states (i.e. it is dynamic).

In supervised learning there is a fixed i.i.d distribution from which the data points are drawn (this is the common assumption at least). In RL there is no fixed distribution, rather this distribution depends on the policy that is followed and often this distribution is not i.i.d but rather correlated.


Hence, RL is a much richer problem than supervised learning. In fact, it is possible to convert any supervised learning task into a reinforcement learning task: the loss function of the supervised task can be used as to define a reward function, with smaller losses mapping to larger rewards. Although it is not clear why one would want to do this because it converts the supervised problem into a more difficult reinforcement learning
problem. Reinforcement learning makes fewer assumptions than supervised learning and is therefore in general a harder problem to solve than supervised learning. However, the opposite is not possible, it is in general not possible to convert a reinforcement learning problem into a supervised learning problem.
"
supervised learning,How to train an artificial neural network to play Diablo 2 using visual input?,"I can see that you are worried about how to train the ANN, but this project hides a complexity that you might not be aware of. Object/character recognition on computer games through image processing it's a highly challenging task (not say crazy for FPS and RPG games). I don't doubt of your skills and I'm also not saying it can't be done, but you can easily spend 10x more time working on recognizing stuff than implementing the ANN itself (assuming you already have experience with digital image processing techniques).
I think your idea is very interesting and also very ambitious. At this point you might want to reconsider it. I sense that this project is something you are planning for the university, so if the focus of the work is really ANN you should probably pick another game, something more simple.
I remember that someone else came looking for tips on a different but somehow similar project not too long ago. It's worth checking it out.
On the other hand, there might be better/easier approaches for identifying objects in-game if you're accepting suggestions. But first, let's call this project for what you want it to be: a smart-bot. 
One method for implementing bots accesses the memory of the game client to find relevant information, such as the location of the character on the screen and it's health. Reading computer memory is trivial, but figuring out exactly where in memory to look for is not. Memory scanners like Cheat Engine can be very helpful for this.
Another method, which works under the game, involves manipulating rendering information. All objects of the game must be rendered to the screen. This means that the locations of all 3D objects will eventually be sent to the video card for processing. Be ready for some serious debugging.
In this answer I briefly described 2 methods to accomplish what you want through image processing. If you are interested in them you can find more about them on Exploiting Online Games (chapter 6), an excellent book on the subject.
"
supervised learning,Use scikit-learn to classify into multiple categories,"What you want is called multi-label classification. Scikits-learn can do that. See here: http://scikit-learn.org/dev/modules/multiclass.html.
I'm not sure what's going wrong in your example, my version of sklearn apparently doesn't have WordNGramAnalyzer. Perhaps it's a question of using more training examples or trying a different classifier? Though note that the multi-label classifier expects the target to be a list of tuples/lists of labels.
The following works for me:
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.multiclass import OneVsRestClassifier

X_train = np.array([""new york is a hell of a town"",
                    ""new york was originally dutch"",
                    ""the big apple is great"",
                    ""new york is also called the big apple"",
                    ""nyc is nice"",
                    ""people abbreviate new york city as nyc"",
                    ""the capital of great britain is london"",
                    ""london is in the uk"",
                    ""london is in england"",
                    ""london is in great britain"",
                    ""it rains a lot in london"",
                    ""london hosts the british museum"",
                    ""new york is great and so is london"",
                    ""i like london better than new york""])
y_train = [[0],[0],[0],[0],[0],[0],[1],[1],[1],[1],[1],[1],[0,1],[0,1]]
X_test = np.array(['nice day in nyc',
                   'welcome to london',
                   'hello welcome to new york. enjoy it here and london too'])   
target_names = ['New York', 'London']

classifier = Pipeline([
    ('vectorizer', CountVectorizer(min_n=1,max_n=2)),
    ('tfidf', TfidfTransformer()),
    ('clf', OneVsRestClassifier(LinearSVC()))])
classifier.fit(X_train, y_train)
predicted = classifier.predict(X_test)
for item, labels in zip(X_test, predicted):
    print '%s => %s' % (item, ', '.join(target_names[x] for x in labels))

For me, this produces the output:
nice day in nyc => New York
welcome to london => London
hello welcome to new york. enjoy it here and london too => New York, London

"
supervised learning,TensorFlow: Calculating gradients of regularization loss terms dependent on model input and output,"Thanks for the clarification in your comment, it makes sense.
Your code looks correct to me -- that is the general approach. Calculate total_loss = data_reconstruction_loss + constant * regularization_loss, then calculate the gradient on the total_loss, and backpropagate. A simple way to make sure that it's working without doing a full hyperparameter sweep is to set a=0 and b=0, then gradually increase a from some very small value (e.g., a=1E-10) to a large value (e.g., a=1). You can take big steps, but you should see your train and validation loss change as you sweep across values of a. You can then repeat the same process with b. If everything works out, continue to the hyperparameter sweep.
"
supervised learning,ClassifierChain with Random Forest: Why is np.nan not supported even though Base Estimator handles it?,"Yes this seems to be how they have written this.
If you see the full stack trace, you will see that this code gets called eventually where the check for Nan does happen.
One workaround is to impute it but also create a feature called is_nan so that model knows when it's actually missing and if your model is complex enough it could learn to ignore the imputed value when the is_nan feature is true.
I agree that the class should have supported Nans, so it might be worth filing a bug request.
"
supervised learning,How to deal with label that not included in training set when doing prediction,"You could set a certain threshold for prediction the known classes. Your model should predict from the known classes only if it predicts it with a certain threshold value, otherwise, it will be classified as unknown. 
The other (and less preferable) way to deal with this problem is to have another class called unknown even during training and put some random faces as corresponding examples of this class.
"
supervised learning,How to publish a Spark ML pyspark.ml.PipelineModel object in code repositories?,"As of March 5, 2024 this is currently not possible with models in Foundry. SparkML models need to serialize the models to a shared hadoop path, which model adapters do not have access to.
You can however, serialize model files to a dataset like the below example.
from transforms.api import transform, Input, Output


@transform(
    training_data_input=Input(""INPUT_PATH""),
    model_output=Output(""/path/to/spark/model/dataset""),
)
def compute(training_data_input, model_output):
    training_df = training_data_input.dataframe()
    model = train_model(training_df)
    path = model_output.filesystem().hadoop_path + '/spark_model'
    model.write().overwrite().save(path)

Which can then be later used for inference like:
from transforms.api import transform, Input, Output
from pyspark.ml import PipelineModel


@transform(
    inference_input_dataset=Input(""INPUT_PATH""),
    model_input=Input(""/path/to/spark/model/dataset""),
    inference_output=Output(""OUTPUT"")
)
def compute(inference_input_dataset, model_input, inference_output):
    inference_input_df = inference_input_dataset.dataframe()

    path = model_input.filesystem().hadoop_path + '/spark_model'
    model = PipelineModel.load(path)  # This needs to match your model class

    inferences_df = model.transform(inference_input_df)
    inference_output.write_dataframe(inferences_df)

Palantir is currently working on how to create a default model serializer for this so that Spark models can be used with models, modeling objectives and deployments directly. I will update this stack overflow post when that is released.
"
supervised learning,Getting 【ValueError: Input contains NaN 】 when using package pmdarima,"Downgrading the following packages will resolve this error:
numpy==1.19.3
pandas==1.3.3
pmdarima==1.8.3

"
supervised learning,Selecting kernel and hyperparameters for kernel PCA reduction,"GridSearchCV is capable of doing cross-validation of unsupervised learning (without a y) as can be seen here in documentation:

fit(X, y=None, groups=None, **fit_params)
...
y : array-like, shape = [n_samples] or [n_samples, n_output], optional 
Target relative to X for classification or regression; 
None for unsupervised learning
...


So the only thing that needs to be handled is how the scoring will be done.
The following will happen in GridSearchCV:

The data X will be be divided into train-test splits based on folds defined in cv param

For each combination of parameters that you specified in param_grid, the model will be trained on the train part from the step above and then scoring will be used on test part.

The scores for each parameter combination will be combined for all the folds and averaged. Highest performing parameter combination will be selected.


Now the tricky part is 2. By default, if you provide a 'string' in that, it will be converted to a make_scorer object internally. For 'mean_squared_error' the relevant code is here:
....
neg_mean_squared_error_scorer = make_scorer(mean_squared_error,
                                        greater_is_better=False)
....

which is what you dont want, because that requires y_true and y_pred.
The other option is to make your own custom scorer as discussed here with signature (estimator, X, y). Something like below for your case:
from sklearn.metrics import mean_squared_error
def my_scorer(estimator, X, y=None):
    X_reduced = estimator.transform(X)
    X_preimage = estimator.inverse_transform(X_reduced)
    return -1 * mean_squared_error(X, X_preimage)

Then use it in GridSearchCV like this:
param_grid = [{
        ""gamma"": np.linspace(0.03, 0.05, 10),
        ""kernel"": [""rbf"", ""sigmoid"", ""linear"", ""poly""]
    }]

kpca=KernelPCA(fit_inverse_transform=True, n_jobs=-1) 
grid_search = GridSearchCV(kpca, param_grid, cv=3, scoring=my_scorer)
grid_search.fit(X)

"
supervised learning,What are advantages of Artificial Neural Networks over Support Vector Machines?,"Judging from the examples you provide, I'm assuming that by ANNs, you mean multilayer feed-forward networks (FF nets for short), such as multilayer perceptrons, because those are in direct competition with SVMs.
One specific benefit that these models have over SVMs is that their size is fixed: they are parametric models, while SVMs are non-parametric. That is, in an ANN you have a bunch of hidden layers with sizes h1 through hn depending on the number of features, plus bias parameters, and those make up your model. By contrast, an SVM (at least a kernelized one) consists of a set of support vectors, selected from the training set, with a weight for each. In the worst case, the number of support vectors is exactly the number of training samples (though that mainly occurs with small training sets or in degenerate cases) and in general its model size scales linearly. In natural language processing, SVM classifiers with tens of thousands of support vectors, each having hundreds of thousands of features, is not unheard of.
Also, online training of FF nets is very simple compared to online SVM fitting, and predicting can be quite a bit faster.
All of the above pertains to the general case of kernelized SVMs. Linear SVM are a special case in that they are parametric and allow online learning with simple algorithms such as stochastic gradient descent.
"
supervised learning,Facing an issue while training UNet for Image Segmentation,"You're likely running into the issue that your input image size must be divisible by 2 ** N, where N is the number of filter layers in your model.
Each layer on in the contracting path (or left side) of the U-Net divides the number of pixels in each dimension by two, while allowing the number of filters to increase. On the expansion path, each layer doubles the number of pixels.
However, the number of pixels is always required to be an integer. If you have five pixels, and you divide it by two and round down, you get two pixels. Double that again, and you get four pixels, which doesn't match the five pixel layer anymore.
Comparing this to your example, you have five layers, so all inputs shapes must be divisible by 2 ** 5. But 376 is not divisible by 32.
This is a problem, because of the concatenation step. When combining the input from expansion path with the input from the contracting path, the dimensions must be compatible, except for the axis you're concatenating on. So, if you had shapes of (None, 46, 16, 512) and (None, 46, 16, 512), that would work. However, you have a mismatch in dimension 1.
There are various ways to handle this.

Pad, crop or resize the original image to a multiple of 2 ** N. (This is likely the simplest - it doesn't require modifying this library.)
Within the U-Net, crop one input image to the size of the smaller image before concatenation.
Within the U-Net, add up to 1 row or column of padding at each layer if needed.

"
supervised learning,SKLearn algorithms than handle native NaN values,"
ValueError: Input X contains NaN. RandomForestClassifier does not accept missing values encoded as NaN natively.

Missing value support in ""classical"" SkLearn tree models is a fairly recent addition.
For DecisionTreeClassifier it's available since SkLearn 1.3(.0). For RandomForestClassifier it's available since SkLearn 1.4(.0). See release notes for more details.
Check your SkLearn version (print(sklearn.__version__)), and if it's less than 1.4.0, upgrade your installation.
"
supervised learning,Unexpected keyword argument &#39;use_dora&#39; when attempting to generate summary from Mistral7B fine-tuned LLM,"I had the same issue, simply upgrading the peft library to the latest version solved the problem for me.
"
supervised learning,Target transformation and feature selection. ValueError: Input X contains NaN,"Revised Answer: The error is due to x containing Nan values. The problem can be resolved by updating scikit-learn to the current version 1.4.0, which allows Nan values in RFECV.
Note: The original suggestion was to check for negative values in the target, as np.log() for negative numbers produces nan values, which was not the cause of the problem.
"
supervised learning,Find how many times each value from column A appears in column B in the same table,"What about using COUNT() on the right side of the left joined table?
select e1.emp_id,
       count(e2.emp_id) as subordinates
  from employee as e1
  left join employee as e2
  on e1.emp_id = e2.super_id
group by e1.emp_id

Fiddle: https://www.db-fiddle.com/f/322VZ9CfYjMMtg8i81T2Dd/1
"
supervised learning,Numpy reshape image to column major order is duplicating my picture,"Actually I found out that I misused np.reshape.
Instead of
imarr = imarr.reshape((3, 256, 256), order='F')

I use
imarr = np.transpose(imarr, (2, 0, 1))

to have shape (3, 256, 256) and it works like a charm.
"
supervised learning,Encoding Json response data using jQuery,"Try this : 
(ive changed a bit the display for you to be able to see ) 
http://jsbin.com/IcapedIw/15/edit
   $.each(data.posts, function(i,data){

   var div_data =""<div class='fixed'><div class='left' style='background-image:url(""+data.image+"")'></div><div class='right'><div class='box'>""+data.name+""""+data.bio.replace(/[^a-zA-Z0-9 ]+/g, """")+""""+1+""</div></div></div>"";
   $(div_data).appendTo(""#imgs"");
 });

it's a known problem : (see here)
String.prototype.cleanup = function() {

   return this.replace(/â€¦/g, '…')
              .replace(/â€“/g, '–')
              .replace(/â€™/g, '’')
              .replace(/â€œ/g, '“')
              .replace(/â€/g, '”');
}

"
supervised learning,How can reproduce animation for rolling window over time?,"In the animation you want, only the green and blue rectangles move and the data is constant. So your update() function should reflect it.
So what you need is to set up initial graph the way you desire, create the parts you want to animate (with init())  and then create frames using update():
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from matplotlib.patches import Rectangle
import pandas as pd
from IPython.display import HTML

# create data
df = pd.DataFrame({
    ""TS_24hrs"": np.arange(0, 274),
    ""count""   : np.abs(np.sin(2 * np.pi * np.arange(0, 274) / 7) + np.random.normal(0, 100.1, size=274)) # generate sesonality
})

# create plot
plt.style.use(""ggplot"")  # <-- set overall look
fig, ax = plt.subplots( figsize=(10,4))

# plot data
plt.plot(df['TS_24hrs'], df['count'], 'r-', linewidth=0.5,  label='data or y')

# make graph beautiful
plt.plot([], [], 'g-', label=""Train"", linewidth=8, alpha=0.3) # <-- dummy legend entry
plt.plot([], [], 'b-', label=""Test"", linewidth=8, alpha=0.3)  # <-- dummy legend entry 
plt.xticks([0, 50, 100, 150, 200, 250, df['TS_24hrs'].iloc[-1]], visible=True, rotation=""horizontal"")

plt.title('Plot of data')
plt.ylabel('count', fontsize=15)
plt.xlabel('Timestamp [24hrs]', fontsize=15)
plt.grid(True)
plt.legend(loc=""upper left"")  
fig.tight_layout(pad=1.2)

Y_LIM = 280
TRAIN_WIDTH = 25
TEST_WIDTH = 10

def init():
    rects = [Rectangle((0, 0), TRAIN_WIDTH, Y_LIM, alpha=0.3, facecolor='green'),
             Rectangle((0 + TRAIN_WIDTH, 0), TEST_WIDTH, Y_LIM, alpha=0.3, facecolor='blue')]
    patches = []
    for rect in rects:
            patches.append(ax.add_patch(rect))
    return patches

def update(x_start):
    patches[0].xy = (x_start, 0)
    patches[1].xy = (x_start + TRAIN_WIDTH, 0)    
    return patches


# create ""Train"" and ""Test"" areas
patches = init()

ani = FuncAnimation(
    fig,
    update, 
    frames=np.linspace(0, 230, 40),  # all starting points
    interval=500,
    blit=True)


HTML(ani.to_html5_video())


"
supervised learning,PyTorch SimCSE loss implementation,"Sure.
Your numerator seems fine so I'll vectorize the denominator.
I'll try to stick to your notation as closely as I can:

norm_hi = torch.sqrt(torch.sum(torch.square(h[:, 0, :]), dim=1))
norm_hj_plus = torch.sqrt(torch.sum(torch.square(h[:, 1, :]), dim=1))
norm_hj_minus = torch.sqrt(torch.sum(torch.square(h[:, 2, :]), dim=1))

sim_denom1 = torch.outer(norm_hi, norm_hj_plus) * temp
sim_denom2 = torch.outer(norm_hi, norm_hj_minus) * temp

v1 = h[:, 0, :] @ h[:, 1, :].t() / sim_denom1
v2 = h[:, 0, :] @ h[:, 2, :].t() / sim_denom2

vec_denom = torch.sum(torch.exp(v1) + torch.exp(v2), dim=1)


You can verify that this computes your denominator like this:
print(torch.allclose(loss, -torch.log(num / vec_denom)))

"
supervised learning,How to Phi^-1 transform a variable in R?,"The Phi_inv() function is the inverse of the Gaussian CDF.  This is already implemented in the qnorm() function in the stats package.  Likewise, Phi(), the CDF of the Gaussian distribution, is implemented in the pnorm() function in the stats package.  You can see as much in the online help file for Phi_inv(). Likewise, in the code, you can see how they are defined:
Phi <- function(x) {
  y <- stats::pnorm(q=x)
  return(y)
}

and
Phi_inv <- function(x) {
  y <- stats::qnorm(x)
  return(y)
}

"
supervised learning,DCGAN debugging. Getting just garbage,"So I solved this issue a while ago, but forgot to post an answer on stack overflow. So I will simply post my code here which should work probably pretty good.
Some disclaimer:

I am not quite sure if it works since I did this a year ago
its for 128x128px Images MNIST
It's not a vanilla GAN I used various optimization techniques
If you want to use it you need to change various details, such as the training dataset

Resources:

Multi-Scale Gradients
Instance Noise
Various tricks I used
More tricks


    import torch
    from torch.autograd import Variable
    import torch.nn as nn
    import torch.nn.functional as F
    import torchvision
    import torchvision.transforms as transforms
    from torch.utils.data import DataLoader
    
    import pytorch_lightning as pl
    from pytorch_lightning import loggers
    
    from numpy.random import choice
    
    import os
    from pathlib import Path
    import shutil
    
    from collections import OrderedDict
    
    # custom weights initialization called on netG and netD
    def weights_init(m):
        classname = m.__class__.__name__
        if classname.find('Conv') != -1:
            nn.init.normal_(m.weight.data, 0.0, 0.02)
        elif classname.find('BatchNorm') != -1:
            nn.init.normal_(m.weight.data, 1.0, 0.02)
            nn.init.constant_(m.bias.data, 0)
    
    # randomly flip some labels
    def noisy_labels(y, p_flip=0.05):  # # flip labels with 5% probability
        # determine the number of labels to flip
        n_select = int(p_flip * y.shape[0])
        # choose labels to flip
        flip_ix = choice([i for i in range(y.shape[0])], size=n_select)
        # invert the labels in place
        y[flip_ix] = 1 - y[flip_ix]
        return y
    
    class AddGaussianNoise(object):
        def __init__(self, mean=0.0, std=0.1):
            self.std = std
            self.mean = mean
    
        def __call__(self, tensor):
            tensor = tensor.cuda()
            return tensor + (torch.randn(tensor.size()) * self.std + self.mean).cuda()
    
        def __repr__(self):
            return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)
    
    def resize2d(img, size):
        return (F.adaptive_avg_pool2d(img, size).data).cuda()
    
    def get_valid_labels(img):
        return ((0.8 - 1.1) * torch.rand(img.shape[0], 1, 1, 1) + 1.1).cuda()  # soft labels
    
    def get_unvalid_labels(img):
        return (noisy_labels((0.0 - 0.3) * torch.rand(img.shape[0], 1, 1, 1) + 0.3)).cuda()  # soft labels
    
    class Generator(pl.LightningModule):
        def __init__(self, ngf, nc, latent_dim):
            super(Generator, self).__init__()
            self.ngf = ngf
            self.latent_dim = latent_dim
            self.nc = nc
    
            self.fc0 = nn.Sequential(
                # input is Z, going into a convolution
                nn.utils.spectral_norm(nn.ConvTranspose2d(latent_dim, ngf * 16, 4, 1, 0, bias=False)),
                nn.LeakyReLU(0.2, inplace=True),
                nn.BatchNorm2d(ngf * 16)
            )
    
            self.fc1 = nn.Sequential(
                # state size. (ngf*8) x 4 x 4
                nn.utils.spectral_norm(nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False)),
                nn.LeakyReLU(0.2, inplace=True),
                nn.BatchNorm2d(ngf * 8)
            )
    
            self.fc2 = nn.Sequential(
                # state size. (ngf*4) x 8 x 8
                nn.utils.spectral_norm(nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False)),
                nn.LeakyReLU(0.2, inplace=True),
                nn.BatchNorm2d(ngf * 4)
            )
    
            self.fc3 = nn.Sequential(
                # state size. (ngf*2) x 16 x 16
                nn.utils.spectral_norm(nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False)),
                nn.LeakyReLU(0.2, inplace=True),
                nn.BatchNorm2d(ngf * 2)
            )
    
            self.fc4 = nn.Sequential(
                # state size. (ngf) x 32 x 32
                nn.utils.spectral_norm(nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False)),
                nn.LeakyReLU(0.2, inplace=True),
                nn.BatchNorm2d(ngf)
            )
    
            self.fc5 = nn.Sequential(
                # state size. (nc) x 64 x 64
                nn.utils.spectral_norm(nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False)),
                nn.Tanh()
            )
    
            # state size. (nc) x 128 x 128
    
            # For Multi-Scale Gradient
            # Converting the intermediate layers into images
            self.fc0_r = nn.Conv2d(ngf * 16, self.nc, 1)
            self.fc1_r = nn.Conv2d(ngf * 8, self.nc, 1)
            self.fc2_r = nn.Conv2d(ngf * 4, self.nc, 1)
            self.fc3_r = nn.Conv2d(ngf * 2, self.nc, 1)
            self.fc4_r = nn.Conv2d(ngf, self.nc, 1)
    
        def forward(self, input):
            x_0 = self.fc0(input)
            x_1 = self.fc1(x_0)
            x_2 = self.fc2(x_1)
            x_3 = self.fc3(x_2)
            x_4 = self.fc4(x_3)
            x_5 = self.fc5(x_4)
    
            # For Multi-Scale Gradient
            # Converting the intermediate layers into images
            x_0_r = self.fc0_r(x_0)
            x_1_r = self.fc1_r(x_1)
            x_2_r = self.fc2_r(x_2)
            x_3_r = self.fc3_r(x_3)
            x_4_r = self.fc4_r(x_4)
    
            return x_5, x_0_r, x_1_r, x_2_r, x_3_r, x_4_r
    
    class Discriminator(pl.LightningModule):
        def __init__(self, ndf, nc):
            super(Discriminator, self).__init__()
            self.nc = nc
            self.ndf = ndf
    
            self.fc0 = nn.Sequential(
                # input is (nc) x 128 x 128
                nn.utils.spectral_norm(nn.Conv2d(nc, ndf, 4, 2, 1, bias=False)),
                nn.LeakyReLU(0.2, inplace=True)
            )
    
            self.fc1 = nn.Sequential(
                # state size. (ndf) x 64 x 64
                nn.utils.spectral_norm(nn.Conv2d(ndf + nc, ndf * 2, 4, 2, 1, bias=False)),
                # ""+ nc"" because of multi scale gradient
                nn.LeakyReLU(0.2, inplace=True),
                nn.BatchNorm2d(ndf * 2)
            )
    
            self.fc2 = nn.Sequential(
                # state size. (ndf*2) x 32 x 32
                nn.utils.spectral_norm(nn.Conv2d(ndf * 2 + nc, ndf * 4, 4, 2, 1, bias=False)),
                # ""+ nc"" because of multi scale gradient
                nn.LeakyReLU(0.2, inplace=True),
                nn.BatchNorm2d(ndf * 4)
            )
    
            self.fc3 = nn.Sequential(
                # state size. (ndf*4) x 16 x 16e
                nn.utils.spectral_norm(nn.Conv2d(ndf * 4 + nc, ndf * 8, 4, 2, 1, bias=False)),
                # ""+ nc"" because of multi scale gradient
                nn.LeakyReLU(0.2, inplace=True),
                nn.BatchNorm2d(ndf * 8),
            )
    
            self.fc4 = nn.Sequential(
                # state size. (ndf*8) x 8 x 8
                nn.utils.spectral_norm(nn.Conv2d(ndf * 8 + nc, ndf * 16, 4, 2, 1, bias=False)),
                nn.LeakyReLU(0.2, inplace=True),
                nn.BatchNorm2d(ndf * 16)
            )
    
            self.fc5 = nn.Sequential(
                # state size. (ndf*8) x 4 x 4
                nn.utils.spectral_norm(nn.Conv2d(ndf * 16 + nc, 1, 4, 1, 0, bias=False)),
                nn.Sigmoid()
            )
    
            # state size. 1 x 1 x 1
    
        def forward(self, input, detach_or_not):
            # When we train i ncombination with generator we use multi scale gradient.
            x, x_0_r, x_1_r, x_2_r, x_3_r, x_4_r = input
            if detach_or_not:
                x = x.detach()
    
            x_0 = self.fc0(x)
    
            x_0 = torch.cat((x_0, x_4_r), dim=1)  # Concat Multi-Scale Gradient
            x_1 = self.fc1(x_0)
    
            x_1 = torch.cat((x_1, x_3_r), dim=1)  # Concat Multi-Scale Gradient
            x_2 = self.fc2(x_1)
    
            x_2 = torch.cat((x_2, x_2_r), dim=1)  # Concat Multi-Scale Gradient
            x_3 = self.fc3(x_2)
    
            x_3 = torch.cat((x_3, x_1_r), dim=1)  # Concat Multi-Scale Gradient
            x_4 = self.fc4(x_3)
    
            x_4 = torch.cat((x_4, x_0_r), dim=1)  # Concat Multi-Scale Gradient
            x_5 = self.fc5(x_4)
    
            return x_5
    
    class DCGAN(pl.LightningModule):
    
        def __init__(self, hparams, checkpoint_folder, experiment_name):
            super().__init__()
            self.hparams = hparams
            self.checkpoint_folder = checkpoint_folder
            self.experiment_name = experiment_name
    
            # networks
            self.generator = Generator(ngf=hparams.ngf, nc=hparams.nc, latent_dim=hparams.latent_dim)
            self.discriminator = Discriminator(ndf=hparams.ndf, nc=hparams.nc)
            self.generator.apply(weights_init)
            self.discriminator.apply(weights_init)
    
            # cache for generated images
            self.generated_imgs = None
            self.last_imgs = None
    
            # For experience replay
            self.exp_replay_dis = torch.tensor([])
    
    
        def forward(self, z):
            return self.generator(z)
    
        def adversarial_loss(self, y_hat, y):
            return F.binary_cross_entropy(y_hat, y)
    
        def training_step(self, batch, batch_nb, optimizer_idx):
            # For adding Instance noise for more visit: https://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/
            std_gaussian = max(0, self.hparams.level_of_noise - (
                    (self.hparams.level_of_noise * 2) * (self.current_epoch / self.hparams.epochs)))
            AddGaussianNoiseInst = AddGaussianNoise(std=std_gaussian)  # the noise decays over time
    
            imgs, _ = batch
            imgs = AddGaussianNoiseInst(imgs)  # Adding instance noise to real images
            self.last_imgs = imgs
    
            # train generator
            if optimizer_idx == 0:
                # sample noise
                z = torch.randn(imgs.shape[0], self.hparams.latent_dim, 1, 1).cuda()
    
                # generate images
                self.generated_imgs = self(z)
    
                # ground truth result (ie: all fake)
                g_loss = self.adversarial_loss(self.discriminator(self.generated_imgs, False), get_valid_labels(self.generated_imgs[0]))  # adversarial loss is binary cross-entropy; [0] is the image of the last layer
    
                tqdm_dict = {'g_loss': g_loss}
                log = {'g_loss': g_loss, ""std_gaussian"": std_gaussian}
                output = OrderedDict({
                    'loss': g_loss,
                    'progress_bar': tqdm_dict,
                    'log': log
                })
                return output
    
            # train discriminator
            if optimizer_idx == 1:
                # Measure discriminator's ability to classify real from generated samples
                # how well can it label as real?
                real_loss = self.adversarial_loss(
                    self.discriminator([imgs, resize2d(imgs, 4), resize2d(imgs, 8), resize2d(imgs, 16), resize2d(imgs, 32), resize2d(imgs, 64)],
                                       False), get_valid_labels(imgs))
    
                fake_loss = self.adversarial_loss(self.discriminator(self.generated_imgs, True), get_unvalid_labels(
                    self.generated_imgs[0]))  # how well can it label as fake?; [0] is the image of the last layer
    
                # discriminator loss is the average of these
                d_loss = (real_loss + fake_loss) / 2
    
                tqdm_dict = {'d_loss': d_loss}
                log = {'d_loss': d_loss, ""std_gaussian"": std_gaussian}
                output = OrderedDict({
                    'loss': d_loss,
                    'progress_bar': tqdm_dict,
                    'log': log
                })
                return output
    
        def configure_optimizers(self):
            lr_gen = self.hparams.lr_gen
            lr_dis = self.hparams.lr_dis
            b1 = self.hparams.b1
            b2 = self.hparams.b2
    
            opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr_gen, betas=(b1, b2))
            opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr_dis, betas=(b1, b2))
            return [opt_g, opt_d], []
    
        def backward(self, trainer, loss, optimizer, optimizer_idx: int) -> None:
            loss.backward(retain_graph=True)
    
        def train_dataloader(self):
            # transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),
            #                                 transforms.ToTensor(),
            #                                 transforms.Normalize([0.5], [0.5])])
            # dataset = torchvision.datasets.MNIST(os.getcwd(), train=False, download=True, transform=transform)
            # return DataLoader(dataset, batch_size=self.hparams.batch_size)
            # transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),
            #                                 transforms.ToTensor(),
            #                                 transforms.Normalize([0.5], [0.5])
            #                                 ])
    
            # train_dataset = torchvision.datasets.ImageFolder(
            #     root=""./drive/My Drive/datasets/flower_dataset/"",
            #     # root=""./drive/My Drive/datasets/ghibli_dataset_small_overfit/"",
            #     transform=transform
            # )
            # return DataLoader(train_dataset, num_workers=self.hparams.num_workers, shuffle=True,
            #                   batch_size=self.hparams.batch_size)
    
            transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),
                                            transforms.ToTensor(),
                                            transforms.Normalize([0.5], [0.5])
                                            ])
            train_dataset = torchvision.datasets.ImageFolder(
                root=""ghibli_dataset_small_overfit/"",
                transform=transform
            )
            return DataLoader(train_dataset, num_workers=self.hparams.num_workers, shuffle=True,
                              batch_size=self.hparams.batch_size)
    
        def on_epoch_end(self):
            z = torch.randn(4, self.hparams.latent_dim, 1, 1).cuda()
            # match gpu device (or keep as cpu)
            if self.on_gpu:
                z = z.cuda(self.last_imgs.device.index)
    
            # log sampled images
            sample_imgs = self.generator(z)[0]
            torchvision.utils.save_image(sample_imgs, f'generated_images_epoch{self.current_epoch}.png')
    
            # save model
            if self.current_epoch % self.hparams.save_model_every_epoch == 0:
                trainer.save_checkpoint(
                    self.checkpoint_folder + ""/"" + self.experiment_name + ""_epoch_"" + str(self.current_epoch) + "".ckpt"")
    
    from argparse import Namespace
    
    args = {
        'batch_size': 128, # batch size
        'lr_gen': 0.0003,  # TTUR;learnin rate of both networks; tested value: 0.0002
        'lr_dis': 0.0003,  # TTUR;learnin rate of both networks; tested value: 0.0002
        'b1': 0.5,  # Momentum for adam; tested value(dcgan paper): 0.5
        'b2': 0.999,  # Momentum for adam; tested value(dcgan paper): 0.999
        'latent_dim': 256,  # tested value which worked(in V4_1): 100
        'nc': 3,  # number of color channels
        'ndf': 8,  # number of discriminator features
        'ngf': 8,  # number of generator features
        'epochs': 4,  # the maxima lamount of epochs the algorith should run
        'save_model_every_epoch': 1,  # how often we save our model
        'image_size': 128, # size of the image
        'num_workers': 3,
        'level_of_noise': 0.1,  # how much instance noise we introduce(std; tested value: 0.15 and 0.1
        'experience_save_per_batch': 1,  # this value should be very low; tested value which works: 1
        'experience_batch_size': 50  # this value shouldnt be too high; tested value which works: 50
    }
    hparams = Namespace(**args)
    
    # Parameters
    experiment_name = ""DCGAN_6_2_MNIST_128px""
    dataset_name = ""mnist""
    checkpoint_folder = ""DCGAN/""
    tags = [""DCGAN"", ""128x128""]
    dirpath = Path(checkpoint_folder)
    
    # defining net
    net = DCGAN(hparams, checkpoint_folder, experiment_name)
    
    torch.autograd.set_detect_anomaly(True)
    trainer = pl.Trainer( # resume_from_checkpoint=""DCGAN_V4_2_GHIBLI_epoch_999.ckpt"",
        max_epochs=args[""epochs""],
        gpus=1
    )
    
    trainer.fit(net)

"
supervised learning,Why does RandomForestClassifier not support missing values while DecisionTreeClassifier does in scikit-learn 1.3?,"As pointed out by Ben Reiniger in the comments, people are acutally working on this feature. From scikit-learn's release history I found out that RandomForestClassifier and RandomForestRegressor will support missing values from version 1.4 on: https://scikit-learn.org/dev/whats_new/v1.4.html#sklearn-ensemble
"
supervised learning,Timeseries plotting,"Looking at the figure, it seems as though the x-axis points are not in order, causing lines to join up unordered timepoints. A quick visual fix is to simply turn the lines off by supplying linestyle='.' when you run plt.plot(...).
To fix the underling cause of unordered timepoints, try applying .sort_index(inplace=True) to your dataframe (assuming the index is the time axis), and if you plot that with lines it should look fine.
"
supervised learning,R Shiny apps - What are the possibilities,"To answer your last question first: Yes, this is a feasible project for which a Shiny app can be well suited.
Import your model into the Shiny App Workspace (e.g. with readRDS()).
Shiny already comes with many inputs (add more with custom HTML if needed). One input is fileInput() which allows users to upload data.
Define a format (.csv/.xlsx/etc..) in which your Shiny app expects the user data. Then make sure that the model can handle the uploaded data (preprocess, filter missing/outlier etc.. if needed).
In which form the result is plotted or basically output, you can also let the user decide (further inputs like checkboxInput(), radioButtons(), selectInput() can be helpful).
And no, the user does not need to have R installed.
"
supervised learning,Machine learning entity candidate scoring,"Possible word subsequences of certain maximum length means it considers every word with the 7 words before and 7 after the word?
As I understand it from the documentation, your description is not quite right. Since every possible sequence up to 15 words in length is evaluated, this would include a word with 7 words before and after it, but also that word with 5 words before and 3 after it, etc. (i.e. every possible N-gram between len(1) and len(15). Initial probabilities are derived, overlapping strings are compared and any overlaps with lower probabilities are discarded so that the final candidates are non-overlapping.  
How can the neural net generate a score when its a binary classification entity and non-entity? Or do they mean the probability score for entity?
According to the Google AI Blog, ""for each candidate the scoring neural net assigns a value (between 0 and 1) based on whether it represents a valid entity."" So that would be a probability. 
How to train a binary NER? Like any other NER except replace all entities to type 'entity' and then generate negative samples for non-entity?
Yes but, because this is a perceptron model, many binary classifiers will be trained and each will function as neuron in the model. It is important to note that the classifier only classifies entity/non-entity, not what type of entity it is. The post also discusses automatically generating negative samples by taking a positive sample (marked by a start token and end token in a string) and deliberately including the token before or after that entity. This technique would greatly increase the size of the training data.
How can this model be fast, as they claim, when it processes every word in the text plus 7 words before and after said word?
The computational cost of taking relatively small string (len 15) and fitting it to a model is small. The computational cost of dividing a longer string into substrings of this length is also quite small. Even if the text is 5000 words long (which would be huge for a query of this sort), that's only about 600,000 n-grams to evaluate, and most of those will have very low entity scores. As I understand it, the most significant computational cost of these approaches is training the model. This is where the ""hashed charactergram embedding"" technique discussed in the post is utilized. 
"
supervised learning,Is Mutual Information(MI) an important factor to consider in unsupervised learning (Clustering)?,"You should do a feature important experiment, like this.
https://github.com/ash-wicus-ml/Notebooks/blob/master/XG%20Boost%20-%20Feature%20Importance.ipynb
When you know what your X-variable is, you can run some clustering exercises.
https://github.com/ash-wicus-ml/Notebooks/blob/master/Clustering%20Algorithms%20Compared.ipynb
"
supervised learning,Numerical and Categorical Features in classification problem,"I would make these variables categorical. There are two approaches that you can try to implement depending on your case:

convert them into separate binary variables, where each category represents a unique week number or day of the month (it helps if there are non-linear relationships between dates and target)

extract from this new feature: for example, you can derive features like ""IsWeekend"" or ""IsHoliday"". It will be more helpful (IMO)


"
supervised learning,Adaline Simple Layer Neural Network Class in Python and ValueError: operands could not be broadcast together with shapes,"I fixed up your AdalineGD class: your net_input should be the random_sample selected from the dataset not the dot product of the weights, and the output and cost_ weren't correct. I used the first two labels and all four features of the Iris dataset for example (so you can input as many features as your dataset has and predict two output labels of -1 and 1), and I included total_errors as another measure of performance:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)
df.tail()
_y = df.iloc[:100, 4].values
y = np.zeros((len(_y), 1), dtype=float)
for i in range(len(_y)):
    if _y[i].endswith('setosa'):
        y[i, 0] = 1.0
    elif _y[i].endswith('versicolor'):
        y[i, 0] = -1.0
x = df.iloc[:100, :4].values

class AdalineGD(object):
    def __init__(self, eta, n_iter):
        self.eta = eta
        self.n_iter = n_iter

    def fit(self, X, y):
        self.w_ = np.ones(1 + x.shape[1])*0.1
        self.cost_ = []
        self.total_errors = []
        error = 0
        for i in range(self.n_iter):
            random_sample = np.random.randint(len(y))
            net_input = X[random_sample]
            output = self.eta * (y[random_sample] - self.predict(net_input))
            self.w_[1:] += output * net_input
            self.w_[0] += output
            cost = ((y[random_sample] - self.predict(net_input))**2).sum() / 2.0
            error += int(output != 0.0)
            self.cost_.append(cost)
            self.total_errors.append(error)
        return self

    def net_input(self, X):
        """"""Calculate net input""""""
        return np.dot(X, self.w_[1:]) + self.w_[0]

    def activation(self, X):
        """"""Compute linear activation""""""
        return X

    def predict(self, X):
        """"""Return class label after unit step""""""
        return np.where(self.activation(self.net_input(X)) >= 0.0, 1.0, -1.0)

ada = AdalineGD(eta=0.01, n_iter=300)
ada.fit(x, y)
plt.plot(ada.total_errors)
plt.plot(ada.cost_)
plt.legend(['total errors', 'cost'], loc='center right')
plt.xlabel('iterations')
plt.show()

Outputs:

"
supervised learning,ValueError: Input X contains NaN,"You are not replacing old dataframe with new dataframe.
Use this:
ds = ds.fillna(0)

OR
ds.fillna(0, inplace=True)

"
supervised learning,Why is Association rule learning considered a supervised learning approach?,"I guess it is an open discussion if one considers Association rule learning as an unsupervised or an supervised learning task. While Wikipedia counts it to the group of supervised learning algorithms other resources count them to the class of unsupervised learning algorithms:

As opposed to decision tree and rule set induction, which result in
  classification models, association rule learning is an unsupervised
  learning method, with no class labels assigned to the examples.
Machine Learning and Data Mining - Springer

I suppose it comes down to how the actual learning part is implemented. One could create a dataset of training data - label pairs such as in your example:
{a, b, c}
{a, b, d}
=> a -> b
=> b -> a

Having a couple hundreds or thousand of these pairs one could train a Neural Network to understand the underlying patterns in the dataset with fairly good accuracy as I would suppose. This would then be a Supervised Learning task, where the NN learns from pre-calssified examples.
If on the other hand the algorithm is implemented in such a way that the associations are computed based on: Support - Confidence - Lift - Conviction it would be an Unsupervised Learning task.
"
supervised learning,Combining different dataset of columns as a full csv datat frame,"First, the dataset doesn't have 5 columns.

This dataset consists of features of handwritten numerals (0--9) extracted from a collection of Dutch utility maps. 200 patterns per class (for a total of 2,000 patterns) have been digitized in binary images. These digits are represented in terms of the following six feature sets (files):

mfeat-fou: 76 Fourier coefficients of the character shapes;
mfeat-fac: 216 profile correlations;
mfeat-kar: 64 Karhunen-Love coefficients;
mfeat-pix: 240 pixel averages in 2 x 3 windows;
mfeat-zer: 47 Zernike moments;
mfeat-mor: 6 morphological features.


As you can see the description. These are separate files that have different columns. They only have the same number of rows (2000).
Let's cut to the chase, this code is what do you want.
from mvlearn.datasets import load_UCImultifeature
import numpy as np
import pandas as pd
# Load entire dataset
full_data, full_labels = load_UCImultifeature()

print([full_data[i].shape for i in range(6)])
# [(2000, 76), (2000, 216), (2000, 64), (2000, 240), (2000, 47), (2000, 6)]

df1 = pd.DataFrame(full_data[0])
df2 = pd.DataFrame(full_labels, columns=[""target""])

concat_df = pd.concat([df1, df2], axis=1)
print(concat_df)

In this case, I just only dealt with the one file of the list. So, if you want to concatenate others as well, you can do this by following:
for i in range(1, len(full_data)):
  df = pd.DataFrame(full_data[i])
  concat_df = pd.concat([df, concat_df], axis=1)

print(concat_df) 
#2000 rows × 650 columns

"
supervised learning,Is it possible to use MLJar for making clustering model?,"You are right. K-means features are only used in preprocessing to add new features. In some datasets it helps to get better results.
MLJAR AutoML right now supports only supervised methods. I would love to extend it to unsupervised learning in the future.
"
supervised learning,Distinguishing between structural &amp; nonstructural regressor candidates in N Lassos run sequentially on N synthetic data sets,"Try this out to recreate that first row in your source datasets:
Structural_or_Non <- lapply(datasets, function(j) {j[1, -1]})

Then, just use an lapply with the names function applied to each element in the list you just created like this:
Structural_Variables <- lapply(Structural_or_Non, function(i) {
names(i)[i == 1] })
Nonstructural_Variables <- lapply(Structural_or_Non, function(i) {
  names(i)[i == 0] })

That should do the trick for you.
"
supervised learning,Need for both CV and train-test-split to assess a model&#39;s performance,"It is recommended to split your initial dataset in 3 parts:

Train
Validation (to tune hyperparameters)
Test (for validation)

Even in the case of the small dataset, I would suggest getting a part of your whole dataset for the test part (this part will be common for every algorithm). All other samples should be used for k-fold CV for every algorithm (be very careful with the standard deviation of your metric). After finding the best hyperparameters and choosing the best model on the test, you have to retrain your model on the whole dataset at once (train + val + test) - it will be your final model for production.
"
supervised learning,Is recurrent neural network a reinforcement learning or supervised learning model?,"RNN is always used in supervised learning, because the core functionality of RNN requires labelled data sent in serially.
Now you must have seen RNN in RL too, but the catch is current deep reinforcement learning use the concept of supervised RNN which acts as a good feature vector for agent inside the RL ecosystem.
In simpler terms, the agent, the reward shaping, the environment everything is RL, but the way the deep network in agent learns is using RNN(or CNN or any type of ANN depending upon the problem statement).
So in short RNN always requires labelled data and hence supervised learning, but it can be used in RL environment too.
"
supervised learning,Which ML algorithm can find pairs in two datasets?,"Try some ""nearest neighbors"" algorithm, with the number of neighbors set to 1.
Specifically, you want to perform a ""search"", which is a bit broader concept than ""classification"" or ""regression"". See the sklearn.neighbors.NearestNeighbors class for more ideas.

there are cases where the monetary amounts are off by a few cents or the ""reference"" has whitespace in the complementary bank info

The ""nearest neighbors"" algorithm has a ""threshold"" parameter, which represents similarity cutoff. Find a ""threshold"" parameter value that captures ""a few cents difference"" but doesn't capture ""a dollar or more difference"".
"
supervised learning,SVC with linear kernel incorrect accuracy,"Add a count vectorizer to your train data and use logistic regression model
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score 

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) 

cv = CountVectorizer() 
ctmTr = cv.fit_transform(X_train) 
X_test_dtm = cv.transform(X_test)

model = LogisticRegression() 
model.fit(ctmTr, y_train)

y_pred_class = model.predict(X_test_dtm)

SVC_Accuracy = accuracy_score(y_test)
print(""\n\n\nLinear SVM Accuracy: "", SVC_Accuracy)

the above model definition is something 'equivalent' to this statement
Linear_SVC_classifier = SVC(kernel='linear', random_state=1)  
Linear_SVC_classifier.fit(ctmTr, y_train)

"
supervised learning,what&#39;s the meaning of ^ in this function,"In statistics, that symbol is used to denote an ""estimator"" of a random variable.
"
supervised learning,What are the disadvantages of self-supervised learning in ML?,"I think that the best way to illustrate this problem is to cite the great Yann LeCun:

If intelligence is a cake, the bulk of the cake is unsupervised
learning, the icing on the cake is supervised learning, and the cherry
on the cake is reinforcement learning (RL).

The different types of ML can be very good or not depending on the case. For example, for robotics or autonomous driving problems, RL would be the ideal solution given the nature of these algorithms. However, for a recommender system or a stock price predictor, you could probably find better (and simpler) solutions in supervised and unsupervised learning.
Reinforcement learning is very different from supervised and unsupervised learning in that it needs to be defined in terms of agent, states, and environment, rather than simply data (and labels in the case of supervised learning). Therefore, you will need those elements and define the interactions between them very carefully to train a good and reliable system that, as I mentioned above, might not be the most optimal (or even feasible) solution for the problem you are trying to solve.
"
supervised learning,Machine learning options to detect errors in a large number of sql tables?,"
Null values or empty strings: an ML algorithm will probably not accept such an input
Truncated strings in numbers: ?
String formatted numbers: numbers are always formatted as strings
Weird date formats: an ML system will require huge samples before it learns rules that you can implement in two minutes
Bad or missing references between tables: how could an ML algorithm deal with this ???

IMO, you forget the most important check: values out of the normal range. These ranges can be found by simple statistical observation or by... common sense.
"
supervised learning,How Machine Learning intgreate with Big Data?,"Heres my quick and to the point opinions:

Machine learning methods ""learn"" through a method called gradient descent, which is typically very data inefficient, but very general, easy to implement, and does not need much prior knowledge of the data.  In order for this method to truly shine, you need a lot of data to get accurate models; hence, a lot of machine learning methods need big data.
Machine learning is a field of artificial intelligence aimed to give the machine abilities to learn concepts without explicitly being programmed to do so.  MapReduce is a distributed computing method that can be used to speed up machine learning training, or any computationally heavy tasks.
The main concept of machine learning is which machine learning algorithms should be used for specific tasks.  For example, supervised learning is used for regression and classification and are applied to data sets that are labeled.  Regression algorithms are used to predict continuous variables and classification algorithms are used for categorical variables.  An example of a continuous task is predicting the real estate price of a particular house.  An example of a categorical task would be predicting if the picture is a dog or cat.   In unsupervised learning, the two main families are principal components and clustering and they are mainly used for unlabeled data sets.  Here, the machine must find the optimal segregation of the data.
Reinforcement learning would definitely be the decision making algorithm; RL was designed for optimal control and optimal decision making after all, since its fundamental algorithm is the Bellman Equation.
Decision making is fundamentally based on the problem you're trying to solve.  For example, if I am trying to create cookies, I can either make my cookies taste damn good, but spend a lot more money, or I can make cookies that taste ok but spend significantly less. Depending on the market I am trying to serve, my decision would be significantly different.  
Since the task for either was not properly defined here, I will make some assumptions.  In health care, one big one would be treatment of individuals with kidney failure.  Here, patients need to go to the hospital for 2-3 hours every 2 days to get their blood cleaned medically through a method called dialysis.  Here, we can build a reinforcement learning controller to control the flow rate, medication, etc, of the dialysis process to both shorten the blood cleaning process AND allow the patient to experience less pain.  I have personally worked on this project.  The decision making for the ML algorithm here is the flow rate and medication amount, among other things.  
In a smart city, the agent might instead want to optimize traffic flow or electricity usage.  For traffic flow, the agent's decision making would be when to make which lights red to minimize the overall total wasted time in traffic.  For electricity usage, the agent would want the electricity to travel at the smallest distance so electricity wastage is minimized.
Big data and deep learning's relationship is same as #1, except, replace the ""a lot"" in my last sentence with all.  Deep learning models are highly parameterized, and require an insane amount of data to be fully accurate and usable (assuming your network is sufficiently deep). However, given sufficient data, its accuracy and abilities are undeniable. The figure below from SumoLogic shows an useful visualization of the change in accuracy in the model vs the amount of data fed into different machine learning algorithms.


"
supervised learning,How to match non-equivalent categories in two datasets for record linkage?,"How to match non-equivalent categories in two datasets for record linkage using Python?
Record linkage is the process of identifying and linking records that refer to the same entity across different data sources. One of the challenges of record linkage is to deal with non-equivalent categories, which are categories that have different names or labels but represent the same concept. For example, the category ""USA"" in one dataset might be equivalent to the category ""United States of America"" in another dataset.
There are different methods to match non-equivalent categories in two datasets for record linkage using Python. Here are three possible methods:
1. Use a dictionary to map non-equivalent categories
One simple method is to create a dictionary that maps the non-equivalent categories from one dataset to the other. For example, if we have two datasets with country names, we can create a dictionary like this:
country_map = {
    ""USA"": ""United States of America"",
    ""UK"": ""United Kingdom"",
    ""UAE"": ""United Arab Emirates"",
    # and so on
}

Then, we can use this dictionary to replace the non-equivalent categories in one of the datasets with the corresponding ones in the other dataset. For example, if we want to match the country names in dataset1 with the ones in dataset2, we can do something like this:
# assume dataset1 and dataset2 are pandas dataframes with a column named ""country""
dataset1[""country""] = dataset1[""country""].apply(lambda x: country_map.get(x, x))
# this will replace the non-equivalent categories in dataset1 with the ones in dataset2
# if the category is not in the dictionary, it will keep the original value

This method is easy to implement and fast to execute, but it requires manual creation and maintenance of the dictionary, which can be tedious and error-prone. It also does not handle spelling errors, typos, or variations in the category names.
2. Use fuzzy matching to find similar categories
Another method is to use fuzzy matching, which is a technique that measures the similarity between two strings based on some criteria, such as edit distance, soundex, or n-grams. Fuzzy matching can help to find categories that are similar in spelling or pronunciation, but not exactly the same. For example, the category ""USA"" might be similar to the category ""U.S.A."" or ""US"".
There are different libraries in Python that can perform fuzzy matching, such as fuzzywuzzy, difflib, or jellyfish. For example, using fuzzywuzzy, we can do something like this:
from fuzzywuzzy import process

# assume dataset1 and dataset2 are pandas dataframes with a column named ""country""
# get the unique categories in dataset2
categories = dataset2[""country""].unique()
# for each category in dataset1, find the most similar category in dataset2
dataset1[""country""] = dataset1[""country""].apply(lambda x: process.extractOne(x, categories)[0])
# this will replace the categories in dataset1 with the most similar ones in dataset2
# based on the Levenshtein distance

This method is more flexible and robust than the dictionary method, as it can handle spelling errors, typos, or variations in the category names. However, it is also more computationally expensive and slower, and it might not always find the correct match, especially if the categories are very different in meaning or structure.
3. Use semantic matching using sentence_transformers
A third method is to use semantic matching, which is a technique that measures the similarity between two strings based on their meaning or context, rather than their form or appearance. Semantic matching can help to find categories that are equivalent in concept, but not in expression. For example, the category ""USA"" might be equivalent to the category ""The country with 50 states and a president"".
One way to perform semantic matching in Python is to use the sentence_transformers library, which is a framework that allows us to use pre-trained models to generate sentence embeddings, which are numerical representations of the meaning of sentences. We can then compare the embeddings of the categories using some metric, such as cosine similarity, to find the most similar ones.
For example, using sentence_transformers, we can do something like this:
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# assume dataset1 and dataset2 are pandas dataframes with a column named ""country""
# load a pre-trained model
model = SentenceTransformer(""paraphrase-MiniLM-L6-v2"")
# get the embeddings of the categories in dataset2
categories = dataset2[""country""].unique()
embeddings = model.encode(categories)
# for each category in dataset1, find the most similar category in dataset2
dataset1[""country""] = dataset1[""country""].apply(lambda x: categories[cosine_similarity(model.encode([x]), embeddings).argmax()])
# this will replace the categories in dataset1 with the most similar ones in dataset2
# based on the cosine similarity of their embeddings

This method is more advanced and powerful than the previous methods, as it can capture the semantic equivalence of the categories, even if they are expressed in different ways. However, it is also more complex and resource-intensive, and it requires a suitable pre-trained model that can handle the domain and language of the categories. It might also not always find the correct match, especially if the categories are ambiguous or have multiple meanings.
Useful search queries to learn more about the different topics

Record linkage in Python: This query can help the user find more information about the general concept and methods of record linkage in Python, and learn how to use different libraries and tools to perform it.
How to deal with non-equivalent categories in record linkage: This query can help the user find more information about the specific challenge and solutions of matching non-equivalent categories in record linkage, and learn from different examples and case studies.
Fuzzy matching in Python: This query can help the user find more information about the fuzzy matching technique and how to use different libraries and algorithms to implement it in Python, and learn about its advantages and limitations.
Semantic matching in Python: This query can help the user find more information about the semantic matching technique and how to use different models and frameworks to perform it in Python, and learn about its benefits and challenges.
Sentence transformers for semantic similarity: This query can help the user find more information about the sentence transformers library and how to use it to generate and compare sentence embeddings for semantic similarity, and learn about its features and applications.

"
supervised learning,Deep Learning for Acoustic Emission concrete fracture speciments: regression on-set time and classification of type of failure,"I finally solved, thanks to the fundamental suggestion by @JonNordby, using Sound Event Detection method. We adopted and readapted the code from GitHub YashNita.
I labelled the data according to the following image:

Then, I adopted the method for extracting features from computing the spectrogram of the input signals:

And finally we were able to get a more precise output recognition of the Seismic Event Detection which is directly connected to the Acoustic Emission Event detection, obtaining the following result:

For the moment, only the event recognition phase was done, but it would be simple to readapt also to conduct classification of mode I or mode II of cracking.
"
unsupervised learning,Scoring returning a numpy.core.memmap instead of a numpy.Number in grid search,"Yes, I had a similar case
I fell in love with .memmap-s due to O/S limits on memory allocations and I consider .memmap-s a smart tool for large scale machine-learning, using them in .fit()-s and other sklearn methods. ( GridSearchCV() not being yet the case, due to its adverse effect of pre-allocation of memory on large HyperPARAMETERs' grids with n_jobs = -1 )

How might we ... reproduce ...? As far as I remember, my case was similar and the change from ""ordinary"" numpy.ndarray to a numpy.memmap() started these artifacts. So, if you strive to create one such artificially, wrap your data into a .memmap()-ed representation of array and make it be returned, even while containing a single cell of data, instead of a plain number. One shall receive a view into a .memmap()-ed sub-range of generic array representation of that cell.

Is the change ... a decent solution? Well, I have got rid of the .memmap()-ed wrapper by explicitly returning a cell value, by referencing the result's [0] component. An enforced conversion by.float() seems fine.
"
unsupervised learning,"LaTeX, How to fit a large table in a page","As suggested by Martin Scharrer in a comment to this answer on TeX.SX, one better alternative to the command \resizebox is to use the adjustbox package. Compile the following and then compare with the same code where \begin{adjustbox}{width=\textwidth} and \end{adjustbox} are commented.
Please post a comment if you need further explainations!
\documentclass{article}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{adjustbox}

\begin{document}

\begin{table}[]
  \centering
  \caption{My caption}
  \label{my-label}

  \begin{adjustbox}{width=\textwidth}

    \begin{tabular}{lllll}
Detection Methods & Supervised /Semi-supervised/ Unsupervised & Technique Used & Applications & Technology \\
Statistical & & Gaussian-based detection & Online anomaly detection & Conventional data centres \\
Statistical & & Gaussian-based detection & General & General \\
Statistical & & Regression analysis & Globally-distributed commercial applications & Distributed, Web-based, Application \& System metrics \\
Statistical & & Regression analysis & Web applications & Enterprise web applications and conventional data centre \\
Statistical & & Correlation & Complex enterprise online applications & Distributed System \\
Statistical & & Correlation & Orleans system and distributed cloud computing services & Virtualized, cloud computing and distributed system (Orleans system) \\
Statistical & & Correlation & Hadoop, Olio and RUBiS & Virtualized cloud computing and distributed systems. \\
ĘMachine learning & Supervised & Bayesian classification & Online application & IBM system S-distributed stream processing cluster \\
Machine learning & Unsupervised & Neighbour-based technique (Local Outlier Factor algorithm) & General & Cloud Computing system \\
Machine learning & Semi-supervised & Principle component analysis and Semi-supervised Decision-tree\_ & Institute-wide cloud computing environment & Cloud Computing \\
Statistical & & Regression curve fitting the service time-adapted cumulative distributed function & Online application service & Platform and configuration agnostic \\
& & & & 
    \end{tabular}

  \end{adjustbox}

\end{table}

\end{document}


A different approach if the (too small) font size in the table is the main matter; you may want to rearrange the text in a single cell on more lines within the cell:
\documentclass{article}

\begin{document}

\begin{table}[htp]
  \centering
  \caption{My caption}
  \label{my-label}
{\small %
    \begin{tabular}{p{.18\textwidth}p{.22\textwidth}p{.2\textwidth}p{.2\textwidth}p{.2\textwidth}}
Detection\par Methods & Supervised/\par Semi-supervised/\par Unsupervised & Technique Used & Applications & Technology \\
Statistical & & Gaussian-based detection & Online anomaly detection & Conventional data centres \\
Statistical & & Gaussian-based detection & General & General \\
Statistical & & Regression\par analysis & Globally-distributed commercial applications & Distributed, Web-based, Application \&\par System metrics \\
Statistical & & Regression\par analysis & Web applications & Enterprise web applications and conventional data centre \\
Statistical & & Correlation & Complex\par enterprise online applications & Distributed\par System \\
Statistical & & Correlation & Orleans system and distributed cloud computing services & Virtualized, cloud computing and distributed system (Orleans system) \\
Statistical & & Correlation & Hadoop,\par Olio and RUBiS & Virtualized cloud computing and distributed systems. \\
ĘMachine\par learning & Supervised & Bayesian\par classification & Online\par application & IBM system S-distributed stream\par processing\par cluster \\
Machine\par learning & Unsupervised & Neighbour-based technique (Local Outlier Factor algorithm) & General & Cloud\par Computing\par system \\
Machine\par learning & Semi-supervised & Principle component analysis and Semi-supervised Decision-tree\_ & Institute-wide cloud computing environment & Cloud\par Computing \\
Statistical & & Regression curve fitting the service time-adapted cumulative distributed function & Online\par application service & Platform and configuration agnostic \\
& & & & 
    \end{tabular}%
}%
\end{table}

\end{document}

Here I used {\small ... } and \par, somewhere, to locally avoid word breaking. You should set font size first, as you prefer it, then the width of the five columns, finally locally adjust where necessary.
"
unsupervised learning,Supervised learning? or unsupervised learning? which one is correct?,"The feature maps generated by intermediate layers of a model like ResNet50 during supervised training can be considered part of the supervised learning process, though they don't directly correspond to the target labels.
During supervised learning, the optimization of parameters—including those responsible for generating feature maps—is driven by the loss function that evaluates the model’s predictions against the target labels. The feature maps are not explicitly supervised themselves (there are no direct labels for the feature maps), but their representations are indirectly shaped to improve the final classification outcome.
The intermediate layers, including conv5, learn features that are most relevant to the supervised task (image classification in this case). These features emerge as the model adjusts its weights to minimize the supervised loss, meaning the process that generates the feature maps is inherently tied to the supervised training pipeline.
In unsupervised learning, features would be extracted without reference to any labels, relying instead on intrinsic patterns in the data (e.g., clustering or autoencoders).
In supervised learning, the features are optimized to aid the ultimate supervised objective, even though the feature maps themselves are not directly compared to labels.
Since the generation of these feature maps is influenced by the supervised objective, they should be categorized as results of supervised learning.This is true even though there is no direct supervision at the level of individual feature maps, they are a byproduct of the overall supervised optimization process.
"
unsupervised learning,Deep learning for inferences in sequences,"
I was wondering what is the state-of-the art deep learning model to replace Hidden Markov Models (HMM)

At the moment RNN (Recurrent Neural Network) and LSTM (Long Short Term Memory) based DNNs are state of the art. They are the best for a lot of sequencing problems starting from Named Entity Recognition (https://www.quora.com/What-is-the-current-state-of-the-art-in-Named-Entity-Recognition-NER/answer/Rahul-Vadaga),  Parsing (https://arxiv.org/pdf/1701.00874.pdf) to Machine Translation (https://arxiv.org/pdf/1609.08144.pdf).
These DNNs are also called sequence models (e.g. seq2seq where input as well as output is a sequence like Machine Translation)
""unsupervised pretraining""
The pre-training is not that popular any more (for supervised ML problems) since you can achieve the same results using random restarts with parallelization as you have more (and cheaper) CPUs now.
A recent paper (Optimal Hyperparameters for Deep LSTM-Networks for Sequence Labeling Tasks
by Nils Reimers, and Iryna Gurevych) does a good comparison of various seq2seq for common NLP tasks: https://arxiv.org/pdf/1707.06799.pdf
Definitely worth a read.
"
unsupervised learning,How to train an artificial neural network to play Diablo 2 using visual input?,"I can see that you are worried about how to train the ANN, but this project hides a complexity that you might not be aware of. Object/character recognition on computer games through image processing it's a highly challenging task (not say crazy for FPS and RPG games). I don't doubt of your skills and I'm also not saying it can't be done, but you can easily spend 10x more time working on recognizing stuff than implementing the ANN itself (assuming you already have experience with digital image processing techniques).
I think your idea is very interesting and also very ambitious. At this point you might want to reconsider it. I sense that this project is something you are planning for the university, so if the focus of the work is really ANN you should probably pick another game, something more simple.
I remember that someone else came looking for tips on a different but somehow similar project not too long ago. It's worth checking it out.
On the other hand, there might be better/easier approaches for identifying objects in-game if you're accepting suggestions. But first, let's call this project for what you want it to be: a smart-bot. 
One method for implementing bots accesses the memory of the game client to find relevant information, such as the location of the character on the screen and it's health. Reading computer memory is trivial, but figuring out exactly where in memory to look for is not. Memory scanners like Cheat Engine can be very helpful for this.
Another method, which works under the game, involves manipulating rendering information. All objects of the game must be rendered to the screen. This means that the locations of all 3D objects will eventually be sent to the video card for processing. Be ready for some serious debugging.
In this answer I briefly described 2 methods to accomplish what you want through image processing. If you are interested in them you can find more about them on Exploiting Online Games (chapter 6), an excellent book on the subject.
"
unsupervised learning,Evaluation metric for parameter tuning for outlier detection (unsupervised learning) on time series,"DBSCAN relies on distance measurements to find clusters, thus it is sensitive to the scale and distribution of the data. Even, in your case, you have just one feature vector, I don't think you need to scale it for outlier detection. Just use residual variable in hyper-parameter and final prediction. You may also need to increase eps may be up to 2. So final code would look like this:
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.seasonal import seasonal_decompose
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score
import optuna

# Function to generate time series data
def generate_time_series(n_samples=300, n_outliers=30):
    np.random.seed(np.random.randint(10000))
    t = np.linspace(0, 50, n_samples)
    y = np.cumsum(np.random.randn(n_samples)) + np.sin(t)  # Adding trend and noise
    outlier_indices = np.random.choice(n_samples, n_outliers, replace=False)
    y[outlier_indices] += 15 * np.random.randn(n_outliers)  # Injecting outliers

    return y.reshape(-1, 1), t

# Generate the time series data
y, t = generate_time_series()

# Plot the time series data
plt.figure(figsize=(10, 5))
plt.plot(t, y, label='Time series', color='blue')
plt.xlabel('Time')
plt.ylabel('Value')
plt.title('Generated Time Series Data')
plt.legend()
plt.show()

# Decompose the time series
result = seasonal_decompose(y, period=30, model='additive', two_sided=True)
residual = result.resid

# Handle NaN values in residuals (if any)
non_nan_indices = ~np.isnan(residual).flatten()
residual = residual[non_nan_indices].reshape(-1, 1)
t_residual = t[non_nan_indices]

# Plot the seasonal decomposition
plt.figure(figsize=(10, 5))
plt.subplot(411)
plt.plot(t, y, label='Original', color='blue')
plt.legend(loc='best')
plt.subplot(412)
plt.plot(t, result.trend, label='Trend', color='orange')
plt.legend(loc='best')
plt.subplot(413)
plt.plot(t, result.seasonal, label='Seasonal', color='green')
plt.legend(loc='best')
plt.subplot(414)
plt.plot(t_residual, residual, label='Residual', color='red')
plt.legend(loc='best')
plt.tight_layout()
plt.show()


# Define the objective function for DBSCAN
def dbscan_objective(trial):
    eps = trial.suggest_float('eps', 0.01, 2, log=True)
    min_samples = trial.suggest_int('min_samples', 2, 20)
    
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    clusters = dbscan.fit_predict(residual)
    
    # Ignore cases where all points are considered noise
    if len(set(clusters)) <= 1:
        return -1.0
    
    score = silhouette_score(residual, clusters)
    return score

# Optimize DBSCAN using Optuna
optuna.logging.set_verbosity(optuna.logging.WARNING)
dbscan_study = optuna.create_study(direction='maximize')
dbscan_study.optimize(dbscan_objective, n_trials=100, show_progress_bar=True)
best_dbscan_params = dbscan_study.best_params
print(f""Best DBSCAN parameters: {best_dbscan_params}"")

# Apply DBSCAN with the best parameters
dbscan = DBSCAN(**best_dbscan_params)
dbscan_clusters = dbscan.fit_predict(residual)
dbscan_outliers = (dbscan_clusters == -1)

# Plot the detected outliers in the residuals
plt.figure(figsize=(10, 5))
plt.plot(t_residual, residual, label='Residual', color='blue')
plt.scatter(t_residual[dbscan_outliers], residual[dbscan_outliers], color='red', label='Outliers')
plt.xlabel('Time')
plt.ylabel('Value')
plt.title('DBSCAN Outlier Detection on Residuals')
plt.legend()
plt.show()

# Plot the detected outliers in the original time series
plt.figure(figsize=(10, 5))
plt.plot(t, y, label='Time series', color='blue')
plt.scatter(t_residual[dbscan_outliers], y[non_nan_indices][dbscan_outliers], color='red', label='Outliers')
plt.xlabel('Time')
plt.ylabel('Value')
plt.title('DBSCAN Outlier Detection on Original Time Series')
plt.legend()
plt.show()

# Print the number of outliers detected by DBSCAN
print(f""Number of outliers detected by DBSCAN: {np.sum(dbscan_outliers)}"")

And you will get something like this:

"
unsupervised learning,Selecting kernel and hyperparameters for kernel PCA reduction,"GridSearchCV is capable of doing cross-validation of unsupervised learning (without a y) as can be seen here in documentation:

fit(X, y=None, groups=None, **fit_params)
...
y : array-like, shape = [n_samples] or [n_samples, n_output], optional 
Target relative to X for classification or regression; 
None for unsupervised learning
...


So the only thing that needs to be handled is how the scoring will be done.
The following will happen in GridSearchCV:

The data X will be be divided into train-test splits based on folds defined in cv param

For each combination of parameters that you specified in param_grid, the model will be trained on the train part from the step above and then scoring will be used on test part.

The scores for each parameter combination will be combined for all the folds and averaged. Highest performing parameter combination will be selected.


Now the tricky part is 2. By default, if you provide a 'string' in that, it will be converted to a make_scorer object internally. For 'mean_squared_error' the relevant code is here:
....
neg_mean_squared_error_scorer = make_scorer(mean_squared_error,
                                        greater_is_better=False)
....

which is what you dont want, because that requires y_true and y_pred.
The other option is to make your own custom scorer as discussed here with signature (estimator, X, y). Something like below for your case:
from sklearn.metrics import mean_squared_error
def my_scorer(estimator, X, y=None):
    X_reduced = estimator.transform(X)
    X_preimage = estimator.inverse_transform(X_reduced)
    return -1 * mean_squared_error(X, X_preimage)

Then use it in GridSearchCV like this:
param_grid = [{
        ""gamma"": np.linspace(0.03, 0.05, 10),
        ""kernel"": [""rbf"", ""sigmoid"", ""linear"", ""poly""]
    }]

kpca=KernelPCA(fit_inverse_transform=True, n_jobs=-1) 
grid_search = GridSearchCV(kpca, param_grid, cv=3, scoring=my_scorer)
grid_search.fit(X)

"
unsupervised learning,"ValueError: Error when checking model target: expected convolution2d_2 to have shape (None, 26, 26, 64) but got array with shape (250, 227, 227, 1)","The problems lied in:

Wrong input_shape - data should be cropped to a video 5-d format. It was done be reshaping and cropping.
Adding TimeDistributed to conv and deconv layers.
Changing a deconv output shape to appropriate values.
Changing border_mode to same.

All other details might be found in comments under the question.
"
unsupervised learning,Unsupervised learning using TSNE and Kmeans,"You fit the scaler with the features, which have 8 columns, and then you tried to inverse transform on the tsne data, which has 2 columns.
To quickly solve this, apply the scaling on the tsne data:
tsne = TSNE(n_components=2, random_state=42)
data_tsne = tsne.fit_transform(data_scaled)
data_scaled = scaler.fit_transform(data_tsne)

I got the following when doing so:

"
unsupervised learning,Learning: KMeans clustering inconsistent results,"
Your result seems consistent to me. Every time you run K-Means, you get the same centroids. The only change is the order, but this should be arbitrary. There's no special reason to assign a particular cluster the attribute of being the first one or second, or third one...

In order to evaluate it, since your data is labeled (iris dataset). I would recommend to check how many items from each cluster correspond to the same labeled set, or how many items with the same label are in the same cluster. For example: are all Iris setosa in the same cluster or are they distributed in more than one cluster?


I guess you precision/recall/F1 if you want, but should define first which cluster correspond to each species. I would start by a visual evaluation, since you have only three tags. But basically, you want a correlation between clusters and species (can cluster predict species?).
But in general, remember that KMeans forces structure in your data, even if there's not (it was originally though as a compression algorithm, not as a cauterization one). So, in many cases you don't actually evaluate it's performance, but just whether is useful or not (for example for feature generation).
PS (after some additional research):
There are two measures that can be quite useful for evaluating clustering if you know the ground truth: Mutual information, and the Adjusted Rand index. MI measures the gain in information about one variable, knowing the other one (how many yes/no questions about one variable can you answer by knowing the other one). Is a non-linear measure of correlation between variables. The Rand index is a measure of the similarity between two data clusters (similar to an accuracy metric), and the adjusted version is corrected against grouping by chance. Both mutual information and adjusted Rand index can be implemented with sklearn.
"
unsupervised learning,How to strip headers/footers from Project Gutenberg texts?,"You weren't kidding. It's almost as if they were trying to make the job AI-complete. I can think of only two approaches, neither of them perfect.
1) Set up a script in, say, Perl, to tackle the most common patterns (e.g., look for the phrase ""produced by"", keep going down to the next blank line and cut there) but put in lots of assertions about what's expected (e.g. the next text should be the title or author). That way when the pattern fails, you'll know it. The first time a pattern fails, do it by hand. The second time, modify the script.
2) Try Amazon's Mechanical Turk.
"
unsupervised learning,How to use a custom scoring function in GridSearchCV for unsupervised learning,"There is no predict method but you can make a custom one.
One approach is to iterate through the core points and assign your new point to the cluster of the first core point that falls within a specified margin, denoted as 'eps.' This ensures that your point will, at the very least, be classified as a border point for the cluster it's assigned to, based on the clustering definitions in use.
import scipy as sp, numpy as np
from sklearn.datasets import make_blobs
from sklearn.model_selection import GridSearchCV
from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score, make_scorer

def dbscan_predict(dbscan_model, X_new, metric=sp.spatial.distance.cosine):
    # Result is noise by default
    y_new = np.ones(shape=len(X_new), dtype=int)*-1 

    # Iterate all input samples for a label
    for j, x_new in enumerate(X_new):
        # Find a core sample closer than EPS
        for i, x_core in enumerate(dbscan_model.components_): 
            if metric(x_new, x_core) < dbscan_model.eps:
                # Assign label of x_core to x_new
                y_new[j] = dbscan_model.labels_[dbscan_model.core_sample_indices_[i]]
                break

    return y_new

def my_custom_function(model, X, y=None):
    # for models that implement it, e.g. KMeans, could use `predict` instead
    preds = dbscan_predict(model, X)
    return silhouette_score(X, preds) if len(set(preds)) > 1 else float('nan')

model = DBSCAN()
pgrid = {
    'eps': [0.1*i for i in range(1,8)],
    'min_samples': range(2,5)
}

Z, _ = make_blobs(400, random_state=0)
gs = GridSearchCV(model, pgrid, scoring=my_custom_function)
gs.fit(Z)
best_estimator = gs.best_estimator_
best_score = gs.score(Z)


ref: https://stackoverflow.com/a/35458920/5025009
"
unsupervised learning,Keras Variational Autoencoder with ImageDataGenerator returns InvalidArgumentError: Graph execution error,"With the help of the MksimSH reponse I resolve the problem that was in the loss function code:
def vae_loss(vae_input,vae_ouput,mu,log_var,kl_coefficient, input_count):
  #Reconstruction loss
  reconstruction_loss = keras.losses.mean_squared_error(vae_input,vae_ouput) * input_count

  #Regularization loss
  kl_loss = 0.5 * K.sum(K.square(mu) + K.exp(log_var) - log_var - 1, axis = -1)

  #Combined loss
  return reconstruction_loss + kl_coefficient*kl_loss

I have to change the shapes of input layers because  keras.losses.mean_squared_error() do not compute a single mean value for 2D data, instead it returns an array of values where each value is a mean of the input row. So the final working code is:
def vae_loss(vae_input,vae_ouput,mu,log_var,kl_coefficient, input_count):
  #Reconstruction loss
  x = keras.layers.Reshape((input_count,))(vae_input)
  y = keras.layers.Reshape((input_count,))(vae_ouput)
  reconstruction_loss = keras.losses.mean_squared_error(x, y) * input_count

  #Regularization loss
  kl_loss = 0.5 * K.sum(K.square(mu) + K.exp(log_var) - log_var - 1, axis = -1)

  #Combined loss
  return reconstruction_loss + kl_coefficient*kl_loss

"
unsupervised learning,Integrate GridSearchCV with LDA Gensim,"
I can not really recognize a question here. Is your current implementation not working?

The package OCTIS (Optimizing and Comparing Topic models Is Simple) is specifically made for this. Could be useful.

Topic modeling metrics are somewhat debated at the moment. There is some research on finding a metric that described how good a topic is. Coherence is traditionally the most used. However, the gold standards for topic quality are metrics that are decided by humans. More specifically word intrusion (showing a topic + one word that is not supposed to be in the topic. And the human needs to pick which one) and topic observed coherence (rating on a 3-point scale).


Depending on what the purpose of the model is, you could use a combination of metrics to decide the best model. Or you could decide by manual inspection what you deem to be the best model.
If you are interested, some papers:
Machine Reading Tea Leaves: Automatically Evaluating Topic Coherence and Topic Model Quality
Is Automated Topic Model Evaluation Broken?: The Incoherence of Coherence
"
unsupervised learning,Is Mutual Information(MI) an important factor to consider in unsupervised learning (Clustering)?,"You should do a feature important experiment, like this.
https://github.com/ash-wicus-ml/Notebooks/blob/master/XG%20Boost%20-%20Feature%20Importance.ipynb
When you know what your X-variable is, you can run some clustering exercises.
https://github.com/ash-wicus-ml/Notebooks/blob/master/Clustering%20Algorithms%20Compared.ipynb
"
unsupervised learning,Scikit Learn GridSearchCV without cross validation (unsupervised learning),"After much searching, I was able to find this thread. It appears that you can get rid of cross validation in GridSearchCV if you use:
cv=[(slice(None), slice(None))]
I have tested this against my own coded version of grid search without cross validation and I get the same results from both methods. I am posting this answer to my own question in case others have the same issue.
Edit: to answer jjrr's question in the comments, here is an example use case:
from sklearn.metrics import silhouette_score as sc

def cv_silhouette_scorer(estimator, X):
    estimator.fit(X)
    cluster_labels = estimator.labels_
    num_labels = len(set(cluster_labels))
    num_samples = len(X.index)
    if num_labels == 1 or num_labels == num_samples:
        return -1
    else:
        return sc(X, cluster_labels)

cv = [(slice(None), slice(None))]
gs = GridSearchCV(estimator=sklearn.cluster.MeanShift(), param_grid=param_dict, 
                  scoring=cv_silhouette_scorer, cv=cv, n_jobs=-1)
gs.fit(df[cols_of_interest])

"
unsupervised learning,TFX&#39;s Evaluator Component cannot prepare the inputs for evaluation,"One thing that I found about the TFMA library and TFX's Evaluator component, in general, is that the output key has to be one-dimensional, and there has to be a label key alway.
If you want to make it work for auto-encoders, instead of making changes to the _input_fn, in the Transform component, return the input twice with two different keys.
For example, if your input key for an image is img, return img_input and img_output in your Transform component. This way, you don't need to manipulate the Trainer component's input_fn, and in the Evaluator, you can easily use the img_output key as your label.
However, as mentioned earlier, this img_output has to be one-dimensional. if in your model, you're using Conv2D layers to encode and decode your image, I'd recommend using the one-dimensional data at first but adding a Reshape layer to make it ready for subsequent Conv2D layers.
Example:
    encoder_inputs = tf.keras.Input(shape=(60,), name='input_xf')
    x = layers.Reshape((15, 4))(encoder_inputs)
    x = layers.Conv1D(filter_num*2, 3, activation=""relu"",
                      strides=2, padding=""valid"")(x)
    z = layers.Dense(latent_dim)(x)

    encoder = tf.keras.Model(encoder_inputs, [z], name=""encoder"")

    latent_inputs = tf.keras.Input(shape=(latent_dim,))
    x = layers.Conv1DTranspose(4, 3, padding=""same"")(x)
    decoder_outputs = layers.Reshape((60,))(x)
    decoder = tf.keras.Model(latent_inputs, decoder_outputs, name=""decoder"")


"
unsupervised learning,Is it possible to use MLJar for making clustering model?,"You are right. K-means features are only used in preprocessing to add new features. In some datasets it helps to get better results.
MLJAR AutoML right now supports only supervised methods. I would love to extend it to unsupervised learning in the future.
"
unsupervised learning,"Child class with different signatures, how to reasonable resolve it without breaking the code?","You can inherit two different classes from your base class: ClassificationEstimator and ClusteringEstimator. After that you can move your fit function to both classes, removing the fit function from base class, and remove the y parameter from the fit function in ClusteringEstimator. After that you could inherit your estimator classes from these two base classes.
In this way your classifier classes will not change but clustering classes will be changed and also the usages shall be updated. By the way you could add an overload for fit function in ClusteringEstimator base class, that takes the y function as an optional non usable parameter. But by the way, leaving the y parameter in fit function is not optimal, because it pollute the library interface.
"
unsupervised learning,Is recurrent neural network a reinforcement learning or supervised learning model?,"RNN is always used in supervised learning, because the core functionality of RNN requires labelled data sent in serially.
Now you must have seen RNN in RL too, but the catch is current deep reinforcement learning use the concept of supervised RNN which acts as a good feature vector for agent inside the RL ecosystem.
In simpler terms, the agent, the reward shaping, the environment everything is RL, but the way the deep network in agent learns is using RNN(or CNN or any type of ANN depending upon the problem statement).
So in short RNN always requires labelled data and hence supervised learning, but it can be used in RL environment too.
"
unsupervised learning,How to use KMeans clustering to improve the accuracy of a logistic regression model?,"I'm having a hard time understanding the context of your problem based on the snippet you provided. Strong work for providing minimal code, but in this case I feel it may have been a bit too minimal. Regardless, I'm going to read between the lines and state some relevent ideas. I'll then attempt to answer your questions more directly.

I am working on a binary classification problem. I have implemented a logistic regression model with an average accuracy of around 75%

This only tells a small amount of the story. knowing what data your classifying and it's general form is pretty vital, and accuracy doesn't tell us a lot about how innaccuracy is distributed through the problem.
Some natural questions:

Is one class 50% accurate and another class is 100% accurate? are the classes both 75% accurate?
what is the class balance? (is there more of one class than the other)?
how much overlap do these classes have?

I recommend profiling your training and testing set, and maybe running your data through TSNE to get an idea of class overlap in your vector space.

these plots will give you an idea of how much overlap your two classes have. In essence, TSNE maps a high dimensional X to a 2d X while attempting to preserve proximity. You can then plot your flagged Y values as color and the 2d X values as points on a grid to get an idea of how tightly packed your classes are in high dimensional space. In the image above, this is a very easy classification problem as each class exists in it's own island. The more these islands mix together, the harder classification will be.

did a grid search to find the best parameters

hot take, but don't use grid search, random search is better. (source Artificial Intelligence by Jones and Barlett). Grid search repeats too much information, wasting time re-exploring similar parameters.

I tried using KMeans clustering, and I set the n_clusters into 2. I trained the logistic regression model using the X_train and y_train values. After that, I tried testing the model on the training data using cross-validation but I set the cross-validation to be against the labels predicted by the KMeans:

So, to rephrase, you trained your model to predict an output given some input, then tested how it performed predicting the same data and got 75%. This is called training accuracy (as opposed to validation or test accuracy). A low training accuracy is indicative of one of two things:

there's a lot of overlap between your classes. If this is the case, I would look into feature engineering. Find a vector space which better segregates the two classes.
there's not a lot of overlap, but the front between the two classes is complex. You need a model with more parameters to segregate your two classes.

model complexity isn't free though. See the curse of dimensionality and overfitting.

ok, answering more directly

these accuracy scores mean your model isn't complex enough to learn the problem, or there's too much overlap between the two classes to see a better accuracy.

I wouldn't use k-means clustering to try to improve this. k-means attempts to find cluster information based on location in a vector space, but you already have flagged data y_train so you already know which clusters data should belong in. Try modifying X_train in some way to get better segregation, or try a more complex model. you can use things like k-means or TSNE to check your transformed X_train for better segregation, but I wouldn't use them directly. Obligatory reminder that you need to test and validate with holdout data. see another answer I provided for more info.

I'd need more code to figure that one out.


p.s. welcome to stack overflow! Keep at it.
"
unsupervised learning,Unsupervised Learning for regression analysis,"The solution inhere for you is to make data out of the signal data. I was also working on similar kind of problem where I was to predict the intensity of fall and data that I got was signal data having x,y,z axis. I managed to solve the problem by initially creating the data using clustering methodology according to my use case.Now since I have supervised data I proceded with futher analysis and predictions.
"
unsupervised learning,Machine learning options to detect errors in a large number of sql tables?,"
Null values or empty strings: an ML algorithm will probably not accept such an input
Truncated strings in numbers: ?
String formatted numbers: numbers are always formatted as strings
Weird date formats: an ML system will require huge samples before it learns rules that you can implement in two minutes
Bad or missing references between tables: how could an ML algorithm deal with this ???

IMO, you forget the most important check: values out of the normal range. These ranges can be found by simple statistical observation or by... common sense.
"
unsupervised learning,How Machine Learning intgreate with Big Data?,"Heres my quick and to the point opinions:

Machine learning methods ""learn"" through a method called gradient descent, which is typically very data inefficient, but very general, easy to implement, and does not need much prior knowledge of the data.  In order for this method to truly shine, you need a lot of data to get accurate models; hence, a lot of machine learning methods need big data.
Machine learning is a field of artificial intelligence aimed to give the machine abilities to learn concepts without explicitly being programmed to do so.  MapReduce is a distributed computing method that can be used to speed up machine learning training, or any computationally heavy tasks.
The main concept of machine learning is which machine learning algorithms should be used for specific tasks.  For example, supervised learning is used for regression and classification and are applied to data sets that are labeled.  Regression algorithms are used to predict continuous variables and classification algorithms are used for categorical variables.  An example of a continuous task is predicting the real estate price of a particular house.  An example of a categorical task would be predicting if the picture is a dog or cat.   In unsupervised learning, the two main families are principal components and clustering and they are mainly used for unlabeled data sets.  Here, the machine must find the optimal segregation of the data.
Reinforcement learning would definitely be the decision making algorithm; RL was designed for optimal control and optimal decision making after all, since its fundamental algorithm is the Bellman Equation.
Decision making is fundamentally based on the problem you're trying to solve.  For example, if I am trying to create cookies, I can either make my cookies taste damn good, but spend a lot more money, or I can make cookies that taste ok but spend significantly less. Depending on the market I am trying to serve, my decision would be significantly different.  
Since the task for either was not properly defined here, I will make some assumptions.  In health care, one big one would be treatment of individuals with kidney failure.  Here, patients need to go to the hospital for 2-3 hours every 2 days to get their blood cleaned medically through a method called dialysis.  Here, we can build a reinforcement learning controller to control the flow rate, medication, etc, of the dialysis process to both shorten the blood cleaning process AND allow the patient to experience less pain.  I have personally worked on this project.  The decision making for the ML algorithm here is the flow rate and medication amount, among other things.  
In a smart city, the agent might instead want to optimize traffic flow or electricity usage.  For traffic flow, the agent's decision making would be when to make which lights red to minimize the overall total wasted time in traffic.  For electricity usage, the agent would want the electricity to travel at the smallest distance so electricity wastage is minimized.
Big data and deep learning's relationship is same as #1, except, replace the ""a lot"" in my last sentence with all.  Deep learning models are highly parameterized, and require an insane amount of data to be fully accurate and usable (assuming your network is sufficiently deep). However, given sufficient data, its accuracy and abilities are undeniable. The figure below from SumoLogic shows an useful visualization of the change in accuracy in the model vs the amount of data fed into different machine learning algorithms.


"
unsupervised learning,Distributed Unsupervised Learning in SageMaker,"SageMaker Training allow you to bring your own training scripts, and supports various forms of distributed training, like data/model parallel, and frameworks like PyTorch DDP, Horovod, DeepSpeed, etc.
Additionally, if you want to bring your data, but not code, SageMaker training offers various unsupervised built-in algorithms, some of which are parallelizable.
"
unsupervised learning,Does correlation important factor in Unsupervised learning (Clustering)?,"My assumption here is that you're asking this question because in cases of linear modeling, highly collinear variables can cause issues.
The short answer is no, you don't need to remove highly correlated variables from clustering for collinearity concerns.  Clustering doesn't rely on linear assumptions, and so collinearity wouldn't cause issues.
That doesn't mean that using a bunch of highly correlated variables is a good thing.  Your features may be overly redundant and you may be using more data than you need to reach the same patterns.  With your data size/feature set that's probably not an issue, but for large data you could leverage the correlated variables via PCA/dimensionality reduction to reduce your computation overhead.
"
unsupervised learning,"supervised learning,unsupervised learning ,regression","1) Linear Regression is Supervised because the data you have include both the input and the output (so to say). So, for instance, if you have a dataset for, say, car sales at a dealership. You have, for each car, the make, model, price, color, discount etc. but you also have the number of sales for each car. If this task was unsupervised, you would have a dataset that included, maybe, just the make, model, price, color etc. (not the actual number of sales) and the best you could do is cluster the data. The example isn't perfect but aims to get across the big picture. A good question to ask yourself when deciding whether a method is supervised or not is to ask ""Do I have a way of adjudging the quality of an input?"". If you have Linear Regression data, you most certainly can. You just evaluate the value of the function (in this case, the line)  for the input data to estimate the output. Not so in the other case.
2) Logistic Regression isn't actually a regression. The name is misleading and does indeed lead to much confusion. It is usually only used for binary prediction which makes it perfect for classification tasks but nothing else.
"
unsupervised learning,What is weakly supervised learning (bootstrapping)?,"Updated answer
As several comments below mention, the situation is not as simple as I originally wrote in 2013.
The generally accepted view is that

weak supervision - supervision with noisy labels (wikipedia)
semi supervision - only a subset of training data has labels (wikipedia)

There are also classifications that are more along with my original answer, for example, Zhi-Hua Zhou's 2017 A brief introduction to weakly supervised learning considers weak supervision to be an umbrella term for

incomplete supervision - only a subset of training data has labels (same as above)
inexact supervision - called  where the training data are given with only coarse-grained labels
inaccurate supervision - where the given labels are not always ground-truth (weak supervision above).


Original answer
In short: In weakly supervised learning, you use a limited amount of labeled data.
How you select this data, and what exactly you do with it depends on the method.  In general you use a limited number of data that is easy to get and/or makes a real difference and then learn the rest. I consider bootstrapping to be a method that can be used in weakly supervised learning, but as the comment by Ben below shows, this is not a generally accepted view.
See, for example Chris Bieman's 2007 dissertation for a nice overview, it says the following about bootstrapping/weakly-supervised learning:

Bootstrapping, also called self-training, is a form of learning that
is designed to use even less training examples, therefore sometimes
called weakly-supervised. Bootstrapping starts with a few training
examples, trains a classifier, and uses thought-to-be positive
examples as yielded by this classifier for retraining. As the set of
training examples grows, the classifier improves, provided that not
too many negative examples are misclassified as positive, which could
lead to deterioration of performance.

For example, in case of part-of-speech tagging, one usually trains an HMM (or maximum-entropy or whatever) tagger on 10,000's words, each with it's POS. In the case of weakly supervised tagging, you might simply use a very small corpus of 100s words. You get some tagger, you use it to tag a corpus of 1000's words, you train a tagger on that and use it to tag even bigger corpus. Obviously, you have to be smarter than this, but this is a good start. (See this paper for a more advance example of a bootstrapped tagger)
Note: weakly supervised learning can also refer to learning with noisy labels (such labels can but do not need to be the result of bootstrapping)
"
unsupervised learning,What is the difference between labeled and unlabeled data?,"Typically, unlabeled data consists of samples of natural or human-created artifacts that you can obtain relatively easily from the world. Some examples of unlabeled data might include photos, audio recordings, videos, news articles, tweets, x-rays (if you were working on a medical application), etc. There is no ""explanation"" for each piece of unlabeled data -- it just contains the data, and nothing else.
Labeled data typically takes a set of unlabeled data and augments each piece of that unlabeled data with some sort of meaningful ""tag,"" ""label,"" or ""class"" that is somehow informative or desirable to know. For example, labels for the above types of unlabeled data might be whether this photo contains a horse or a cow, which words were uttered in this audio recording, what type of action is being performed in this video, what the topic of this news article is, what the overall sentiment of this tweet is, whether the dot in this x-ray is a tumor, etc.
Labels for data are often obtained by asking humans to make judgments about a given piece of unlabeled data (e.g., ""Does this photo contain a horse or a cow?"") and are significantly more expensive to obtain than the raw unlabeled data.
After obtaining a labeled dataset, machine learning models can be applied to the data so that new unlabeled data can be presented to the model and a likely label can be guessed or predicted for that piece of unlabeled data.
There are many active areas of research in machine learning that are aimed at integrating unlabeled and labeled data to build better and more accurate models of the world. Semi-supervised learning attempts to combine unlabeled and labeled data (or, more generally, sets of unlabeled data where only some data points have labels) into integrated models. Deep neural networks and feature learning are areas of research that attempt to build models of the unlabeled data alone, and then apply information from the labels to the interesting parts of the models.
"
unsupervised learning,Split text into sentences without NLTK,"I agree with Tim that using NLTK is the correct solution. That said, the reason your existing code isn't working is because you put a capturing group in your regex, and re.split will include the capture groups in the result, not just the strings outside the regex entirely, per the docs:

If capturing parentheses are used in pattern, then the text of all groups in the pattern are also returned as part of the resulting list.

You ended up capturing a single space character between each of the sentences, adding seven ""sentences"" that were all length one strings.
The minimal fix is to stop re.split from preserving it by making it a non-capturing group with ?:, making it (?:\s|\n):
sentences = re.split(r""(?<=[^A-Z].[.!?])(?:\s|\n)+(?=[A-Z])"", text)

but in this particular case you don't need a group at all, the escapes are legal in character classes, so you can just use [\s\n], which is both more succinct and potentially more performant (due to lower complexity of classes over grouping). And in fact, \s already includes \n as a component (\n is a whitespace character), so you don't even need the explicit character class at all:
sentences = re.split(r""(?<=[^A-Z].[.!?])\s+(?=[A-Z])"", text)

Note that in both cases:

I've placed a r prefix on the regex to make it a raw-string literal; you got lucky here (the invalid str literal escape \s is ignored with a DeprecationWarning you're not opted in to see, and \n happens to produce a raw newline character, which re treats as equivalent to \n), but some day you'll want to use \b for a word boundary, and if you're not using raw-string literals you'll be very confused when nothing matches (because the str literal converted the \b to an ASCII backspace, and re never knew it was looking for a word boundary). ALWAYS use raw strings for regex, and you won't get bitten.
I've added ! to the character class in the lookbehind assertion (you say you want to allow sentences to end in ., ! or ?, but you were only allowing . or ?).


Update: In response to your edited example input where the last logical sentence isn't split from the second-to-last, the reason the regex doesn't split As of 2020, many sources continue to assert that ML remains a subfield of AI. from Others have the view that not all ML is part of AI, but only an 'intelligent subset' of ML should be considered AI. is because of your positive lookbehind assertion, (?<=[^A-Z].[.?]). Expanding that, the assertion translates to:
(?<=[^A-Z].[.?])
 ^^^           ^ The splitter must be preceded by
    ^^^^^^       a character that is not an uppercase ASCII alphabetic character
          ^      followed by any character
           ^^^^  followed by a period or a question mark

The first logical sentence here ends with AI.; the I matches the ., the . matches [.?], but A fails the [^A-Z] test.
You could just make the look-behind assertion simplify to just the end of sentence character, e.g.:
sentences = re.split(r""(?<=[.!?])\s+(?=[A-Z])"", text)
#                      ^^^^^^^^^^ Only checking that the whitespace is preceded by ., ! or ?

and that will split your sentences completely, but it's unclear if the additional components of that look-behind assertion are important, e.g. if you meant to prevent initials from being interpreted as sentence boundaries (if I cite S. Ranger that will be interpreted as a sentence boundary after S., but if I narrow it to exclude that case, then Who am I? I am I. Or am I? will decide both I? and I. are not sentence boundaries).
"
unsupervised learning,Automatically built regex expressions that fit set of strings,"OK, we'll try to break this down into manageable steps.
  1. For each substring w in s1, in order of non-increasing length,
  2.  assume w is a substring of the other sM
  3.  for each string of the other sN,
  4.   if w is not a substring of sN, disprove assumption and break
  5.  if the assumption held, save w
  6.  if you've found three w that work, break
  7. You have recorded between 0 and 3 w that work.

Note that not all sets of strings are guaranteed to have common substrings (except the empty string). In the worst case, assume s1 is the longest string. There are O(n^2) substrings of s1 (|s1| = n) and it takes O(n) to compare to each of m other strings... so the asymptotic complexity is, I believe, O(n^2 * nm)... even though the algorithm is naive, this should be pretty manageable (polynomial, after all, and quadratic at that).
The transformation to e.g. C code should be straightforward... use a sliding window with a decrementing length loop to get substrings of s1, and then use linear searchers to find matches in the other strings.
I'm sure there are smarter / asymptotically better ways of doing this, but any algorithm will have to look at all characters in all strings, so O(nm)... may not be completely right here.
"
unsupervised learning,Coupling of Different Blocks in a UNET,"the problem is very simple, you are concatenating block with layers with different size, this is happening because you are trying to run the network on images that are NOT POWER OF 2 size, when you do the max pooling of an image that is not divisible for 2 you lose a pixel (243x243 -> 121x121) and when you double with the traspose you get a different size (121x121 -> 242x242) and the concatenation doesnt work because 242 is different to 243, the images are of different size (at least this is what i think, you should have shared the error).
This means that when an image reaches a maxpooling layer it needs to have an edge divisible for 2.
so, solution:
having 4 blocks means that the images need to be AT LEAST divisible for 16, otherwise it will not work
"
unsupervised learning,Unsupervised learning in artificial neural networks,"Genetic algorithm itself is an optimization algorithm rather than a learning algorithm. And you probably don't want to ignore the performance of neural network and only consider the weight changes. So what type of learning is a combination of neural network and genetic algorithm depends on the learning type of neural network. A neural network can be used for supervised learning, reinforcement learning, and even unsupervised learning.
It seems such a combination applies more in reinforcement, because genetic algorithm is slower than most backpropagation-based optimization algorithms with gradient information. Updating the weights with genetic algorithm is also called neuroevolution. This post cited many research on neuroevolution over traditional neural network in case you may be interested in.
It is also possible to apply genetic algorithm to unsupervised neural network. In Shibata et al's Nonlinear backlash compensation using recurrent neural network. Unsupervised learning by genetic algorithm, the authors applied the genetic algorithm to determine the weights of the recurrent neural networks, and the approach does not need the teaching signals.
"
unsupervised learning,Clustering images using unsupervised Machine Learning,"Label a few examples, and use classification.
Clustering is as likely to give you the clusters ""images with a blueish tint"", ""grayscale scans"" and ""warm color temperature"". That is a quote reasonable way to cluster such images.
Furthermore, k-means is very sensitive to outliers. And you probably have some in there.
Since you want your clusters correspond to certain human concepts, classification is what you need to use.
"
unsupervised learning,Is there a way to calculate cosine similarity between documents sets in Python?,"I think this does what you are looking for:
def most_similar(doc_id, similarity_matrix, matrix):
    print(similarity_matrix)
    print(f'Document: {documents_df.iloc[doc_id][""documents""]}')
    print('\n')
    print('Similar Documents:')
    if matrix == 'Cosine Similarity':
        similar_ix = similarity_matrix[doc_id][::-1]
    elif matrix == 'Euclidean Distance':
        similar_ix = similarity_matrix[doc_id]
    for i, ix in enumerate(similar_ix):
        if ix == doc_id:
            continue
        print('\n')
        print(f'Document: {documents_df.iloc[i][""documents""]}')
        print(f'{matrix} : {similarity_matrix[doc_id][i]}')

most_similar(0, pairwise_similarities, 'Cosine Similarity')
most_similar(0, pairwise_differences, 'Euclidean Distance')

"
unsupervised learning,When to use supervised or unsupervised learning?,"
If you a have labeled dataset you can use both. If you have no labels you only can use unsupervised learning. 
It´s not a question of ""better"". It´s a question of what you want to achieve. E.g. clustering data is usually unsupervised – you want the algorithm to tell you how your data is structured. Categorizing is supervised since you need to teach your algorithm what is what in order to make predictions on unseen data. 
See 1.

On a side note: These are very broad questions. I suggest you familiarize yourself with some ML foundations. 
Good podcast for example here: http://ocdevel.com/podcasts/machine-learning
Very good book / notebooks by Jake VanderPlas: http://nbviewer.jupyter.org/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb
"
unsupervised learning,R not displaying Arabic text correctly,"If you're using old a version of R that is 3.2 or Less then those
versions does not handle Unicode in proper way. Try to install latest
version of R from https://cran.r-project.org/ and if required then
install all packages.
"
unsupervised learning,what is convergence in k Means?,"Ideally, if the values in the last two consequent iterations are same then the algorithm is said to have converged. But often people use a less strict criteria for convergence, like, the difference in the values of last two iterations is less than a particular threshold etc,.
"
unsupervised learning,Can I fine-tune BERT using only masked language model and next sentence prediction?,"Your first approach should be to try the pre-trained weights. Generally it works well. However if you are working on a different domain (e.g.: Medicine), then you'll need to fine-tune on data from new domain. Again you might be able to find pre-trained models on the domains (e.g.: BioBERT).
For adding layer, there are slightly different approaches depending on your task. E.g.: For question-answering, have a look at TANDA paper (Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection). It is a very nice easily readable paper which explains the transfer and adaptation strategy. Again, hugging-face has modified and pre-trained models for most of the standard tasks.
"
unsupervised learning,Getting weights from tensorflow.js neural network,"It seems like there is probably a simpler and cleaner way to do what you want, but regardless:
Calling this.model.getWeights() will give you an array of Variables that correspond to layer weights and biases. Calling data() on any of these array elements will return a promise that you can resolve to get the weights. 
I haven't tried manually setting the weights, but there is a this.model.setWeights() method. 
Goodluck.  
"
unsupervised learning,Fitting data vs. transforming data in scikit-learn,"Fitting finds the internal parameters of a model that will be used to transform data. Transforming  applies the parameters to data. You may fit a model to one set of data, and then transform it on a completely different set.
For example, you fit a linear model to data to get a slope and intercept. Then you use those parameters to transform (i.e., map) new or existing values of x to y.
fit_transform is just doing both steps to the same data.
A scikit example: You fit data to find the principal components. Then you transform your data to see how it maps onto these components:
from sklearn.decomposition import PCA

pca = PCA(n_components=2)

X = [[1,2],[2,4],[1,3]]

pca.fit(X)

# This is the model to map data
pca.components_

array([[ 0.47185791,  0.88167459],
       [-0.88167459,  0.47185791]], dtype=float32)

# Now we actually map the data
pca.transform(X)

array([[-1.03896057, -0.17796634],
       [ 1.19624651, -0.11592512],
       [-0.15728599,  0.29389156]])

# Or we can do both ""at once""
pca.fit_transform(X)

array([[-1.03896058, -0.1779664 ],
       [ 1.19624662, -0.11592512],
       [-0.15728603,  0.29389152]], dtype=float32)

"
unsupervised learning,Unsupervised Clustering of large multi-dimentional data,"Since you have trouble with the necessary amount of compute you have to make some sort of compromise here. Here's a few suggestions that will likely fix your problem, but they all come at a cost.

Dimension reduction i.e. PCA to reduce your number of columns to ~2 or so. You will lose some information, but you will be able to plot it and do inference via K-means.

Average the patients data. Not sure if this will be enough, this depends on you data. This will lose the over-time observation of your patients but likely drastically reduce your number of rows.


My suggestion is to do dimension reduction since losing the over time data on your patients might render your data useless. There is also other stuff beside PCA, for example auto encoders. For clustering the way your descibe I'd recommend you stick to K-means or soft K-means.
"
unsupervised learning,Why isn&#39;t DropOut used in Unsupervised Learning?,"Dropout is used in unsupervised learning. For example:

Shuangfei Zhai, Zhongfei Zhang: Dropout Training of Matrix Factorization and Autoencoder for Link Prediction in Sparse Graphs (arxiv, 14 Dec 2015)

"
unsupervised learning,How to verify proper shape of time series with ML,"imho machine learning is overengineering this problem, some banding and counting peaks seems to be the much easier approach to me.
Nontheless if you want machinelearning i would go with autoencoders for anomaly detection, examples can be found here or here.
Tl/dr:
The idea is an autoencoder reconstructs the input though a very small bottleneck (i.e one value, could be the phase) so any current point will construct a good looking curve. Then it gets compared to the actual curve. If it fits all is good, if it doesnt you know something is not right.
"
unsupervised learning,Fast Text unsupervised model loss,"I can't answer all your questions in depth, but I try to give you some advice.

you can understand better avg.loss, reading this thread
learning rate is updated according lrUpdateRate option (read this).
in general, increasing the number of epochs can improve learning. However, as you can read in this paper, the most popular language models have a number of epochs between 10 and 100.
default loss function is softmax. You can also choose hs (hierarchical softmax) or ns. You can read more in the official tutorial.
if you want to learn more about the effects of the ws and wordngrams parameters, you can read this answer.

"
unsupervised learning,How to cast data from long to wide format in H2O?,"I think #2 is your best bet right now, since we don't currently have a function to do that in H2O.  I think this would be a useful utility, so  have created a JIRA ticket for it here.  I don't know when it will get worked on, so I'd still suggesting coding up #2 for the time-being.
The SVMLight/LIBSVM format was originally developed for a particular SVM implementation (as the name suggests), but it's generic and not at all specific to SVM.  If you don't have labeled data, then you can fill in a dummy value where it expects a label.
To export an R data.frame in this format, you can use this package and there is more info here.  You might be able to find better packages for this by searching ""svmlight"" or ""libsvm"" on http://rdocumentation.org.
You can then read in the sparse file directly into H2O using the h2o.importFile() function with parse_type = ""SVMLight"".
"
unsupervised learning,Can a neural network be trained with just a single class of training data?,"Of course it can be. But in this case it will only recognize this one class that you have trained it with. And depending on the expected output you can measure the similarity to the training data. 
An NN, after training, is just a function. For classification problems you can imagine it as a function that takes data as input and returns an integer indicating to which class it belongs to. That being said, if you have only one class that can be represented by an integer value 1, and if training data is not similar to that class, you will get something like 1.555; It will not tel you that it belongs to another class, because you have introduced only one, but it will definitely give you a hint about its similarity.
NNs are considered to be supervised learning, because before training you have to provide both input and target, i. e. the expected output.
"
unsupervised learning,Calculate Cosine Similarity for a word2vec model in R,"following github-code explains how you can use the cosine similarity in Word2Vec Models in R:
https://gist.github.com/adamlauretig/d15381b562881563e97e1e922ee37920
You can use this function at every matrix in R and therefore for every Word2Vec Model built in R.
Kind Regards,
Tom
"
unsupervised learning,How to evaluate unsupervised anomaly detection,"The only way is to generate synthetic anomalies which mean to introduce outliers by yourself with the knowledge of how a typical outlier will look like.
"
unsupervised learning,Feature selection: coarse or fine data,"since the finer grained column contains all the information the coarser grained column does, you can just drop the coarser grained column avoiding correlated features.
However it finally depends on your model if it is bothered by correlated features or not and if it is capable to do the aggregation to the coarser level implicitly (e.g. decision trees can)
"
unsupervised learning,Error: Mean Distance Between Objects Zero,"The error happens if you have certain numeric columns whose mean is 0. You can reproduce the error by turning any 1 column to 0.
data$a <- 0
som <- supersom(data= as.list(data), grid = somgrid(10,10, ""hexagonal""), 
                dist.fct = ""euclidean"", keep.data = TRUE)


Error in supersom(data = as.list(data), grid = somgrid(10, 10, ""hexagonal""),  :
Non-informative layers present: mean distance between objects zero

Maybe you can investigate why those column have 0 mean or remove the columns with 0 means from the data.
library(kohonen)
library(dplyr)

data <- data %>% select(where(~(is.numeric(.) && mean(.) > 0) | !is.numeric(.)))
#som model
som <- supersom(data= as.list(data), grid = somgrid(10,10, ""hexagonal""), 
                dist.fct = ""euclidean"", keep.data = TRUE)

"
unsupervised learning,"When using the K-Means Clustering Algorithm, is it possible to have a set of data which results in an Infinite Loop?","tl;dr No, a K-means algorithm always has an end point if the algorithm is coded correctly.
Explanation:
The ideal way to think about this is not in the sense of what datapoints would cause issues, but rather about how kmeans is working in the broader sense of things. The k-means algorithm is always working in a finite space. For N data points, there are only N ^ k distinct arrangements for the data points. (This number can be pretty large, but is still finite)
Secondly, a k-means algorithm is always optimizing a loss function, based on the sum of squared distances between each data point and it's assigned cluster center. This means two very important things: Each of the N ^ k distinct arrangements can be arranged in an ascending/descending order of minimum loss to maximum loss. Also, the K-means algorithm will never go from a state of lower net loss to a higher net loss. 
These two conditions guarantee that the algorithm will always tend towards the minimum loss arrangement in a finite space, thus ensuring that it has an end.
The last edge case: What if more than one minimum state has equal loss? This is a highly unlikely scenario, but can cause issues if and only if the algorithm is coded poorly for tie breakers. Essentially, the only way this can cause a cycle is if a data point has equal distance for two clusters, and is allowed to change clusters away from it's current cluster even on equal distance. Suffice to say, the algorithms are generally coded so that the data points never swap on a tie, or in some other deterministic manner, thus avoiding this scenario entirely. 
"
unsupervised learning,Underfull \hbox (badness 10000) in table,"A p{<len>} column specification tries to justify any multiline content, stretching it out so it is both flush left and flush right with the column boundary (except for the last line). In your case, there's no way to stretch out Supervised to fit exactly within 0.14\textwidth, causing the ""Underfull \hbox"" warning.
Since that column is so narrow, it's better to force some alignment/spacing using \makecell (from the makecell package). Below I've also used booktabs and tabularx to improve the visual appeal.

\documentclass{article}

\usepackage{booktabs,makecell,tabularx}

\begin{document}

\begin{table}
  \centering
  \caption{Learning Types}
  \begin{tabularx}{\linewidth}{ l X l }
    \toprule
    \thead{Type} & \thead{Description} & \thead{Example(s)} \\
    \midrule
    \makecell[lt]{Supervised \\ Learning} & 
      The data you feed to the algorithm is labeled.
      It is often used for classification and regression &
      \makecell[lt]{%
        $k$-Nearest Neighbors             \\
        Linear Regression                 \\
        Logistic Regression               \\
        Support Vector Machines (SVMs)    \\
        Decision Trees and Random Forests \\
        Neural networks%
      } \\
    \addlinespace[10pt]
    \makecell[lt]{Unsupervised \\ Learning} & Description & Example(s) \\
    \addlinespace[10pt]
    \makecell[lt]{Semi-supervised \\ Learning} & Description & Example(s) \\
    \addlinespace[10pt]
    \makecell[lt]{Reinforcement \\ Learning} & Description & Example(s) \\
    \bottomrule
  \end{tabularx}
\end{table}

\end{document}

"
semi-supervised learning,"LaTeX, How to fit a large table in a page","As suggested by Martin Scharrer in a comment to this answer on TeX.SX, one better alternative to the command \resizebox is to use the adjustbox package. Compile the following and then compare with the same code where \begin{adjustbox}{width=\textwidth} and \end{adjustbox} are commented.
Please post a comment if you need further explainations!
\documentclass{article}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{adjustbox}

\begin{document}

\begin{table}[]
  \centering
  \caption{My caption}
  \label{my-label}

  \begin{adjustbox}{width=\textwidth}

    \begin{tabular}{lllll}
Detection Methods & Supervised /Semi-supervised/ Unsupervised & Technique Used & Applications & Technology \\
Statistical & & Gaussian-based detection & Online anomaly detection & Conventional data centres \\
Statistical & & Gaussian-based detection & General & General \\
Statistical & & Regression analysis & Globally-distributed commercial applications & Distributed, Web-based, Application \& System metrics \\
Statistical & & Regression analysis & Web applications & Enterprise web applications and conventional data centre \\
Statistical & & Correlation & Complex enterprise online applications & Distributed System \\
Statistical & & Correlation & Orleans system and distributed cloud computing services & Virtualized, cloud computing and distributed system (Orleans system) \\
Statistical & & Correlation & Hadoop, Olio and RUBiS & Virtualized cloud computing and distributed systems. \\
ĘMachine learning & Supervised & Bayesian classification & Online application & IBM system S-distributed stream processing cluster \\
Machine learning & Unsupervised & Neighbour-based technique (Local Outlier Factor algorithm) & General & Cloud Computing system \\
Machine learning & Semi-supervised & Principle component analysis and Semi-supervised Decision-tree\_ & Institute-wide cloud computing environment & Cloud Computing \\
Statistical & & Regression curve fitting the service time-adapted cumulative distributed function & Online application service & Platform and configuration agnostic \\
& & & & 
    \end{tabular}

  \end{adjustbox}

\end{table}

\end{document}


A different approach if the (too small) font size in the table is the main matter; you may want to rearrange the text in a single cell on more lines within the cell:
\documentclass{article}

\begin{document}

\begin{table}[htp]
  \centering
  \caption{My caption}
  \label{my-label}
{\small %
    \begin{tabular}{p{.18\textwidth}p{.22\textwidth}p{.2\textwidth}p{.2\textwidth}p{.2\textwidth}}
Detection\par Methods & Supervised/\par Semi-supervised/\par Unsupervised & Technique Used & Applications & Technology \\
Statistical & & Gaussian-based detection & Online anomaly detection & Conventional data centres \\
Statistical & & Gaussian-based detection & General & General \\
Statistical & & Regression\par analysis & Globally-distributed commercial applications & Distributed, Web-based, Application \&\par System metrics \\
Statistical & & Regression\par analysis & Web applications & Enterprise web applications and conventional data centre \\
Statistical & & Correlation & Complex\par enterprise online applications & Distributed\par System \\
Statistical & & Correlation & Orleans system and distributed cloud computing services & Virtualized, cloud computing and distributed system (Orleans system) \\
Statistical & & Correlation & Hadoop,\par Olio and RUBiS & Virtualized cloud computing and distributed systems. \\
ĘMachine\par learning & Supervised & Bayesian\par classification & Online\par application & IBM system S-distributed stream\par processing\par cluster \\
Machine\par learning & Unsupervised & Neighbour-based technique (Local Outlier Factor algorithm) & General & Cloud\par Computing\par system \\
Machine\par learning & Semi-supervised & Principle component analysis and Semi-supervised Decision-tree\_ & Institute-wide cloud computing environment & Cloud\par Computing \\
Statistical & & Regression curve fitting the service time-adapted cumulative distributed function & Online\par application service & Platform and configuration agnostic \\
& & & & 
    \end{tabular}%
}%
\end{table}

\end{document}

Here I used {\small ... } and \par, somewhere, to locally avoid word breaking. You should set font size first, as you prefer it, then the width of the five columns, finally locally adjust where necessary.
"
semi-supervised learning,Deep learning for inferences in sequences,"
I was wondering what is the state-of-the art deep learning model to replace Hidden Markov Models (HMM)

At the moment RNN (Recurrent Neural Network) and LSTM (Long Short Term Memory) based DNNs are state of the art. They are the best for a lot of sequencing problems starting from Named Entity Recognition (https://www.quora.com/What-is-the-current-state-of-the-art-in-Named-Entity-Recognition-NER/answer/Rahul-Vadaga),  Parsing (https://arxiv.org/pdf/1701.00874.pdf) to Machine Translation (https://arxiv.org/pdf/1609.08144.pdf).
These DNNs are also called sequence models (e.g. seq2seq where input as well as output is a sequence like Machine Translation)
""unsupervised pretraining""
The pre-training is not that popular any more (for supervised ML problems) since you can achieve the same results using random restarts with parallelization as you have more (and cheaper) CPUs now.
A recent paper (Optimal Hyperparameters for Deep LSTM-Networks for Sequence Labeling Tasks
by Nils Reimers, and Iryna Gurevych) does a good comparison of various seq2seq for common NLP tasks: https://arxiv.org/pdf/1707.06799.pdf
Definitely worth a read.
"
semi-supervised learning,DCGAN debugging. Getting just garbage,"So I solved this issue a while ago, but forgot to post an answer on stack overflow. So I will simply post my code here which should work probably pretty good.
Some disclaimer:

I am not quite sure if it works since I did this a year ago
its for 128x128px Images MNIST
It's not a vanilla GAN I used various optimization techniques
If you want to use it you need to change various details, such as the training dataset

Resources:

Multi-Scale Gradients
Instance Noise
Various tricks I used
More tricks


    import torch
    from torch.autograd import Variable
    import torch.nn as nn
    import torch.nn.functional as F
    import torchvision
    import torchvision.transforms as transforms
    from torch.utils.data import DataLoader
    
    import pytorch_lightning as pl
    from pytorch_lightning import loggers
    
    from numpy.random import choice
    
    import os
    from pathlib import Path
    import shutil
    
    from collections import OrderedDict
    
    # custom weights initialization called on netG and netD
    def weights_init(m):
        classname = m.__class__.__name__
        if classname.find('Conv') != -1:
            nn.init.normal_(m.weight.data, 0.0, 0.02)
        elif classname.find('BatchNorm') != -1:
            nn.init.normal_(m.weight.data, 1.0, 0.02)
            nn.init.constant_(m.bias.data, 0)
    
    # randomly flip some labels
    def noisy_labels(y, p_flip=0.05):  # # flip labels with 5% probability
        # determine the number of labels to flip
        n_select = int(p_flip * y.shape[0])
        # choose labels to flip
        flip_ix = choice([i for i in range(y.shape[0])], size=n_select)
        # invert the labels in place
        y[flip_ix] = 1 - y[flip_ix]
        return y
    
    class AddGaussianNoise(object):
        def __init__(self, mean=0.0, std=0.1):
            self.std = std
            self.mean = mean
    
        def __call__(self, tensor):
            tensor = tensor.cuda()
            return tensor + (torch.randn(tensor.size()) * self.std + self.mean).cuda()
    
        def __repr__(self):
            return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)
    
    def resize2d(img, size):
        return (F.adaptive_avg_pool2d(img, size).data).cuda()
    
    def get_valid_labels(img):
        return ((0.8 - 1.1) * torch.rand(img.shape[0], 1, 1, 1) + 1.1).cuda()  # soft labels
    
    def get_unvalid_labels(img):
        return (noisy_labels((0.0 - 0.3) * torch.rand(img.shape[0], 1, 1, 1) + 0.3)).cuda()  # soft labels
    
    class Generator(pl.LightningModule):
        def __init__(self, ngf, nc, latent_dim):
            super(Generator, self).__init__()
            self.ngf = ngf
            self.latent_dim = latent_dim
            self.nc = nc
    
            self.fc0 = nn.Sequential(
                # input is Z, going into a convolution
                nn.utils.spectral_norm(nn.ConvTranspose2d(latent_dim, ngf * 16, 4, 1, 0, bias=False)),
                nn.LeakyReLU(0.2, inplace=True),
                nn.BatchNorm2d(ngf * 16)
            )
    
            self.fc1 = nn.Sequential(
                # state size. (ngf*8) x 4 x 4
                nn.utils.spectral_norm(nn.ConvTranspose2d(ngf * 16, ngf * 8, 4, 2, 1, bias=False)),
                nn.LeakyReLU(0.2, inplace=True),
                nn.BatchNorm2d(ngf * 8)
            )
    
            self.fc2 = nn.Sequential(
                # state size. (ngf*4) x 8 x 8
                nn.utils.spectral_norm(nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False)),
                nn.LeakyReLU(0.2, inplace=True),
                nn.BatchNorm2d(ngf * 4)
            )
    
            self.fc3 = nn.Sequential(
                # state size. (ngf*2) x 16 x 16
                nn.utils.spectral_norm(nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False)),
                nn.LeakyReLU(0.2, inplace=True),
                nn.BatchNorm2d(ngf * 2)
            )
    
            self.fc4 = nn.Sequential(
                # state size. (ngf) x 32 x 32
                nn.utils.spectral_norm(nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False)),
                nn.LeakyReLU(0.2, inplace=True),
                nn.BatchNorm2d(ngf)
            )
    
            self.fc5 = nn.Sequential(
                # state size. (nc) x 64 x 64
                nn.utils.spectral_norm(nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False)),
                nn.Tanh()
            )
    
            # state size. (nc) x 128 x 128
    
            # For Multi-Scale Gradient
            # Converting the intermediate layers into images
            self.fc0_r = nn.Conv2d(ngf * 16, self.nc, 1)
            self.fc1_r = nn.Conv2d(ngf * 8, self.nc, 1)
            self.fc2_r = nn.Conv2d(ngf * 4, self.nc, 1)
            self.fc3_r = nn.Conv2d(ngf * 2, self.nc, 1)
            self.fc4_r = nn.Conv2d(ngf, self.nc, 1)
    
        def forward(self, input):
            x_0 = self.fc0(input)
            x_1 = self.fc1(x_0)
            x_2 = self.fc2(x_1)
            x_3 = self.fc3(x_2)
            x_4 = self.fc4(x_3)
            x_5 = self.fc5(x_4)
    
            # For Multi-Scale Gradient
            # Converting the intermediate layers into images
            x_0_r = self.fc0_r(x_0)
            x_1_r = self.fc1_r(x_1)
            x_2_r = self.fc2_r(x_2)
            x_3_r = self.fc3_r(x_3)
            x_4_r = self.fc4_r(x_4)
    
            return x_5, x_0_r, x_1_r, x_2_r, x_3_r, x_4_r
    
    class Discriminator(pl.LightningModule):
        def __init__(self, ndf, nc):
            super(Discriminator, self).__init__()
            self.nc = nc
            self.ndf = ndf
    
            self.fc0 = nn.Sequential(
                # input is (nc) x 128 x 128
                nn.utils.spectral_norm(nn.Conv2d(nc, ndf, 4, 2, 1, bias=False)),
                nn.LeakyReLU(0.2, inplace=True)
            )
    
            self.fc1 = nn.Sequential(
                # state size. (ndf) x 64 x 64
                nn.utils.spectral_norm(nn.Conv2d(ndf + nc, ndf * 2, 4, 2, 1, bias=False)),
                # ""+ nc"" because of multi scale gradient
                nn.LeakyReLU(0.2, inplace=True),
                nn.BatchNorm2d(ndf * 2)
            )
    
            self.fc2 = nn.Sequential(
                # state size. (ndf*2) x 32 x 32
                nn.utils.spectral_norm(nn.Conv2d(ndf * 2 + nc, ndf * 4, 4, 2, 1, bias=False)),
                # ""+ nc"" because of multi scale gradient
                nn.LeakyReLU(0.2, inplace=True),
                nn.BatchNorm2d(ndf * 4)
            )
    
            self.fc3 = nn.Sequential(
                # state size. (ndf*4) x 16 x 16e
                nn.utils.spectral_norm(nn.Conv2d(ndf * 4 + nc, ndf * 8, 4, 2, 1, bias=False)),
                # ""+ nc"" because of multi scale gradient
                nn.LeakyReLU(0.2, inplace=True),
                nn.BatchNorm2d(ndf * 8),
            )
    
            self.fc4 = nn.Sequential(
                # state size. (ndf*8) x 8 x 8
                nn.utils.spectral_norm(nn.Conv2d(ndf * 8 + nc, ndf * 16, 4, 2, 1, bias=False)),
                nn.LeakyReLU(0.2, inplace=True),
                nn.BatchNorm2d(ndf * 16)
            )
    
            self.fc5 = nn.Sequential(
                # state size. (ndf*8) x 4 x 4
                nn.utils.spectral_norm(nn.Conv2d(ndf * 16 + nc, 1, 4, 1, 0, bias=False)),
                nn.Sigmoid()
            )
    
            # state size. 1 x 1 x 1
    
        def forward(self, input, detach_or_not):
            # When we train i ncombination with generator we use multi scale gradient.
            x, x_0_r, x_1_r, x_2_r, x_3_r, x_4_r = input
            if detach_or_not:
                x = x.detach()
    
            x_0 = self.fc0(x)
    
            x_0 = torch.cat((x_0, x_4_r), dim=1)  # Concat Multi-Scale Gradient
            x_1 = self.fc1(x_0)
    
            x_1 = torch.cat((x_1, x_3_r), dim=1)  # Concat Multi-Scale Gradient
            x_2 = self.fc2(x_1)
    
            x_2 = torch.cat((x_2, x_2_r), dim=1)  # Concat Multi-Scale Gradient
            x_3 = self.fc3(x_2)
    
            x_3 = torch.cat((x_3, x_1_r), dim=1)  # Concat Multi-Scale Gradient
            x_4 = self.fc4(x_3)
    
            x_4 = torch.cat((x_4, x_0_r), dim=1)  # Concat Multi-Scale Gradient
            x_5 = self.fc5(x_4)
    
            return x_5
    
    class DCGAN(pl.LightningModule):
    
        def __init__(self, hparams, checkpoint_folder, experiment_name):
            super().__init__()
            self.hparams = hparams
            self.checkpoint_folder = checkpoint_folder
            self.experiment_name = experiment_name
    
            # networks
            self.generator = Generator(ngf=hparams.ngf, nc=hparams.nc, latent_dim=hparams.latent_dim)
            self.discriminator = Discriminator(ndf=hparams.ndf, nc=hparams.nc)
            self.generator.apply(weights_init)
            self.discriminator.apply(weights_init)
    
            # cache for generated images
            self.generated_imgs = None
            self.last_imgs = None
    
            # For experience replay
            self.exp_replay_dis = torch.tensor([])
    
    
        def forward(self, z):
            return self.generator(z)
    
        def adversarial_loss(self, y_hat, y):
            return F.binary_cross_entropy(y_hat, y)
    
        def training_step(self, batch, batch_nb, optimizer_idx):
            # For adding Instance noise for more visit: https://www.inference.vc/instance-noise-a-trick-for-stabilising-gan-training/
            std_gaussian = max(0, self.hparams.level_of_noise - (
                    (self.hparams.level_of_noise * 2) * (self.current_epoch / self.hparams.epochs)))
            AddGaussianNoiseInst = AddGaussianNoise(std=std_gaussian)  # the noise decays over time
    
            imgs, _ = batch
            imgs = AddGaussianNoiseInst(imgs)  # Adding instance noise to real images
            self.last_imgs = imgs
    
            # train generator
            if optimizer_idx == 0:
                # sample noise
                z = torch.randn(imgs.shape[0], self.hparams.latent_dim, 1, 1).cuda()
    
                # generate images
                self.generated_imgs = self(z)
    
                # ground truth result (ie: all fake)
                g_loss = self.adversarial_loss(self.discriminator(self.generated_imgs, False), get_valid_labels(self.generated_imgs[0]))  # adversarial loss is binary cross-entropy; [0] is the image of the last layer
    
                tqdm_dict = {'g_loss': g_loss}
                log = {'g_loss': g_loss, ""std_gaussian"": std_gaussian}
                output = OrderedDict({
                    'loss': g_loss,
                    'progress_bar': tqdm_dict,
                    'log': log
                })
                return output
    
            # train discriminator
            if optimizer_idx == 1:
                # Measure discriminator's ability to classify real from generated samples
                # how well can it label as real?
                real_loss = self.adversarial_loss(
                    self.discriminator([imgs, resize2d(imgs, 4), resize2d(imgs, 8), resize2d(imgs, 16), resize2d(imgs, 32), resize2d(imgs, 64)],
                                       False), get_valid_labels(imgs))
    
                fake_loss = self.adversarial_loss(self.discriminator(self.generated_imgs, True), get_unvalid_labels(
                    self.generated_imgs[0]))  # how well can it label as fake?; [0] is the image of the last layer
    
                # discriminator loss is the average of these
                d_loss = (real_loss + fake_loss) / 2
    
                tqdm_dict = {'d_loss': d_loss}
                log = {'d_loss': d_loss, ""std_gaussian"": std_gaussian}
                output = OrderedDict({
                    'loss': d_loss,
                    'progress_bar': tqdm_dict,
                    'log': log
                })
                return output
    
        def configure_optimizers(self):
            lr_gen = self.hparams.lr_gen
            lr_dis = self.hparams.lr_dis
            b1 = self.hparams.b1
            b2 = self.hparams.b2
    
            opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr_gen, betas=(b1, b2))
            opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr_dis, betas=(b1, b2))
            return [opt_g, opt_d], []
    
        def backward(self, trainer, loss, optimizer, optimizer_idx: int) -> None:
            loss.backward(retain_graph=True)
    
        def train_dataloader(self):
            # transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),
            #                                 transforms.ToTensor(),
            #                                 transforms.Normalize([0.5], [0.5])])
            # dataset = torchvision.datasets.MNIST(os.getcwd(), train=False, download=True, transform=transform)
            # return DataLoader(dataset, batch_size=self.hparams.batch_size)
            # transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),
            #                                 transforms.ToTensor(),
            #                                 transforms.Normalize([0.5], [0.5])
            #                                 ])
    
            # train_dataset = torchvision.datasets.ImageFolder(
            #     root=""./drive/My Drive/datasets/flower_dataset/"",
            #     # root=""./drive/My Drive/datasets/ghibli_dataset_small_overfit/"",
            #     transform=transform
            # )
            # return DataLoader(train_dataset, num_workers=self.hparams.num_workers, shuffle=True,
            #                   batch_size=self.hparams.batch_size)
    
            transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),
                                            transforms.ToTensor(),
                                            transforms.Normalize([0.5], [0.5])
                                            ])
            train_dataset = torchvision.datasets.ImageFolder(
                root=""ghibli_dataset_small_overfit/"",
                transform=transform
            )
            return DataLoader(train_dataset, num_workers=self.hparams.num_workers, shuffle=True,
                              batch_size=self.hparams.batch_size)
    
        def on_epoch_end(self):
            z = torch.randn(4, self.hparams.latent_dim, 1, 1).cuda()
            # match gpu device (or keep as cpu)
            if self.on_gpu:
                z = z.cuda(self.last_imgs.device.index)
    
            # log sampled images
            sample_imgs = self.generator(z)[0]
            torchvision.utils.save_image(sample_imgs, f'generated_images_epoch{self.current_epoch}.png')
    
            # save model
            if self.current_epoch % self.hparams.save_model_every_epoch == 0:
                trainer.save_checkpoint(
                    self.checkpoint_folder + ""/"" + self.experiment_name + ""_epoch_"" + str(self.current_epoch) + "".ckpt"")
    
    from argparse import Namespace
    
    args = {
        'batch_size': 128, # batch size
        'lr_gen': 0.0003,  # TTUR;learnin rate of both networks; tested value: 0.0002
        'lr_dis': 0.0003,  # TTUR;learnin rate of both networks; tested value: 0.0002
        'b1': 0.5,  # Momentum for adam; tested value(dcgan paper): 0.5
        'b2': 0.999,  # Momentum for adam; tested value(dcgan paper): 0.999
        'latent_dim': 256,  # tested value which worked(in V4_1): 100
        'nc': 3,  # number of color channels
        'ndf': 8,  # number of discriminator features
        'ngf': 8,  # number of generator features
        'epochs': 4,  # the maxima lamount of epochs the algorith should run
        'save_model_every_epoch': 1,  # how often we save our model
        'image_size': 128, # size of the image
        'num_workers': 3,
        'level_of_noise': 0.1,  # how much instance noise we introduce(std; tested value: 0.15 and 0.1
        'experience_save_per_batch': 1,  # this value should be very low; tested value which works: 1
        'experience_batch_size': 50  # this value shouldnt be too high; tested value which works: 50
    }
    hparams = Namespace(**args)
    
    # Parameters
    experiment_name = ""DCGAN_6_2_MNIST_128px""
    dataset_name = ""mnist""
    checkpoint_folder = ""DCGAN/""
    tags = [""DCGAN"", ""128x128""]
    dirpath = Path(checkpoint_folder)
    
    # defining net
    net = DCGAN(hparams, checkpoint_folder, experiment_name)
    
    torch.autograd.set_detect_anomaly(True)
    trainer = pl.Trainer( # resume_from_checkpoint=""DCGAN_V4_2_GHIBLI_epoch_999.ckpt"",
        max_epochs=args[""epochs""],
        gpus=1
    )
    
    trainer.fit(net)

"
semi-supervised learning,What are the disadvantages of self-supervised learning in ML?,"I think that the best way to illustrate this problem is to cite the great Yann LeCun:

If intelligence is a cake, the bulk of the cake is unsupervised
learning, the icing on the cake is supervised learning, and the cherry
on the cake is reinforcement learning (RL).

The different types of ML can be very good or not depending on the case. For example, for robotics or autonomous driving problems, RL would be the ideal solution given the nature of these algorithms. However, for a recommender system or a stock price predictor, you could probably find better (and simpler) solutions in supervised and unsupervised learning.
Reinforcement learning is very different from supervised and unsupervised learning in that it needs to be defined in terms of agent, states, and environment, rather than simply data (and labels in the case of supervised learning). Therefore, you will need those elements and define the interactions between them very carefully to train a good and reliable system that, as I mentioned above, might not be the most optimal (or even feasible) solution for the problem you are trying to solve.
"
semi-supervised learning,Machine learning options to detect errors in a large number of sql tables?,"
Null values or empty strings: an ML algorithm will probably not accept such an input
Truncated strings in numbers: ?
String formatted numbers: numbers are always formatted as strings
Weird date formats: an ML system will require huge samples before it learns rules that you can implement in two minutes
Bad or missing references between tables: how could an ML algorithm deal with this ???

IMO, you forget the most important check: values out of the normal range. These ranges can be found by simple statistical observation or by... common sense.
"
semi-supervised learning,Underfull \hbox (badness 10000) in table,"A p{<len>} column specification tries to justify any multiline content, stretching it out so it is both flush left and flush right with the column boundary (except for the last line). In your case, there's no way to stretch out Supervised to fit exactly within 0.14\textwidth, causing the ""Underfull \hbox"" warning.
Since that column is so narrow, it's better to force some alignment/spacing using \makecell (from the makecell package). Below I've also used booktabs and tabularx to improve the visual appeal.

\documentclass{article}

\usepackage{booktabs,makecell,tabularx}

\begin{document}

\begin{table}
  \centering
  \caption{Learning Types}
  \begin{tabularx}{\linewidth}{ l X l }
    \toprule
    \thead{Type} & \thead{Description} & \thead{Example(s)} \\
    \midrule
    \makecell[lt]{Supervised \\ Learning} & 
      The data you feed to the algorithm is labeled.
      It is often used for classification and regression &
      \makecell[lt]{%
        $k$-Nearest Neighbors             \\
        Linear Regression                 \\
        Logistic Regression               \\
        Support Vector Machines (SVMs)    \\
        Decision Trees and Random Forests \\
        Neural networks%
      } \\
    \addlinespace[10pt]
    \makecell[lt]{Unsupervised \\ Learning} & Description & Example(s) \\
    \addlinespace[10pt]
    \makecell[lt]{Semi-supervised \\ Learning} & Description & Example(s) \\
    \addlinespace[10pt]
    \makecell[lt]{Reinforcement \\ Learning} & Description & Example(s) \\
    \bottomrule
  \end{tabularx}
\end{table}

\end{document}

"
semi-supervised learning,"When I use TF-IDF in Natural language processing, it said list is not callable.Can you help me with it?","From the code you provided nothing seems wrong. However, I hypothesize that somewhere before that block of code, you assigned an object to the variable name list (most likely something along the lines of list = [...]) which is usually the cause of this error. 
Try to find that line of code if it exists and rename that variable. Generally it is not a good idea to rename built-in types for this reason. For more info read this
"
semi-supervised learning,Tensorflow difference between tf.stop_gradient and feed variables to optimizer?,"I found a possible solution to my question and I'm posting it here, in case someone may find it useful.
Apparently, tf.stop_gradients() only stops the new gradients to be back-propagated through the layers, but: if we have a momentum term (e.g. when using Adam or RMSProp) the variables of such layers could still be updated due to some gradients cumulated in the past (contained in the momentum term). Let's have a look at the simple case of SGD + Momentum; the formula would be:
w1 = w0 - a*grad(loss) - b*v0

where w1 and w0 are the weights at time 0 and 1, a is the learning rate v0 is the accumulated velocity (a function of the past gradients). Using tf.stop_gradients() is equivalent to multiplying the second term for zero. Then, the update rule becomes:
w1 = w0 - b*v0

e.g. we still have a momentum component that can update the weights. 
A workaround to this problem would be to explicitly passing the variables to be updated to the optimizer. For example:
var_list = take_all_variables_in_N2()
train_op = tf.train.AdamOptimizer(lr).minimize(loss, var_list)


References:
[1] http://ruder.io/optimizing-gradient-descent/
[2] Using stop_gradient with AdamOptimizer in TensorFlow 
"
semi-supervised learning,When should I use support vector machines as opposed to artificial neural networks?,"Are SVMs better than ANN with many classes? You are probably referring to the fact that SVMs are in essence, either either one-class or two-class classifiers. Indeed they are and there's no way to modify a SVM algorithm to classify more than two classes.
The fundamental feature of a SVM is the separating maximum-margin hyperplane whose position is determined by maximizing its distance from the support vectors. And yet SVMs are routinely used for multi-class classification, which is accomplished with a processing wrapper around multiple SVM classifiers that work in a ""one against many"" pattern--i.e., the training data is shown to the first SVM which classifies those instances as ""Class I"" or ""not Class I"". The data in the second class, is then shown to a second SVM which classifies this data as ""Class II"" or ""not Class II"", and so on. In practice, this works quite well. So as you would expect, the superior resolution of SVMs compared to other classifiers is not limited to two-class data. 
As far as i can tell, the studies reported in the literature confirm this, e.g., In the provocatively titled paper Sex with Support Vector Machines substantially better resolution for sex identification (Male/Female) in 12-square pixel images, was reported for SVM compared with that of a group of traditional linear classifiers; SVM also outperformed RBF NN, as well as large ensemble RBF NN). But there seem to be plenty of similar evidence for the superior performance of SVM in multi-class problems: e.g., SVM outperformed NN in protein-fold recognition, and in time-series forecasting.
My impression from reading this literature over the past decade or so, is that the majority of the carefully designed studies--by persons skilled at configuring and using both techniques, and using data sufficiently resistant to classification to provoke some meaningful difference in resolution--report the superior performance of SVM relative to NN. But as your Question suggests, that performance delta seems to be, to a degree, domain specific.
For instance, NN outperformed SVM in a comparative study of author identification from texts in Arabic script; In a study comparing credit rating prediction, there was no discernible difference in resolution by the two classifiers; a similar result was reported in a  study of high-energy particle classification.
I have read, from more than one source in the academic literature, that SVM outperforms NN as the size of the training data decreases.
Finally, the extent to which one can generalize from the results of these comparative studies is probably quite limited. For instance, in one study comparing the accuracy of SVM and NN in time series forecasting, the investigators reported that SVM did indeed outperform a conventional (back-propagating over layered nodes) NN but performance of the SVM was about the same as that of an RBF (radial basis function) NN.
[Are SVMs better than ANN] In an Online setting? SVMs are not used in an online setting (i.e., incremental training). The essence of SVMs is the separating hyperplane whose position is determined by a small number of support vectors. So even a single additional data point could in principle significantly influence the position of this hyperplane.
What about in a semi-supervised case like reinforcement learning? Until the OP's comment to this answer, i was not aware of either Neural Networks or SVMs used in this way--but they are. 
The most widely used- semi-supervised variant of SVM is named Transductive SVM (TSVM), first mentioned by Vladimir Vapnick (the same guy who discovered/invented conventional SVM). I know almost nothing about this technique other than what's it is called and that is follows the principles of transduction (roughly lateral reasoning--i.e., reasoning from training data to test data). Apparently TSV is a preferred technique in the field of text classification.
Is there a better unsupervised version of SVMs? I don't believe SVMs are suitable for unsupervised learning. Separation is based on the position of the maximum-margin hyperplane determined by support vectors. This could easily be my own limited understanding, but i don't see how that would happen if those support vectors were unlabeled (i.e., if you didn't know before-hand what you were trying to separate). One crucial use case of unsupervised algorithms is when you don't have labeled data or you do and it's badly unbalanced. E.g., online fraud; here you might have in your training data, only a few data points labeled as ""fraudulent accounts"" (and usually with questionable accuracy) versus the remaining >99% labeled ""not fraud."" In this scenario, a one-class classifier, a typical configuration for SVMs, is the a good option. In particular, the training data consists of instances labeled ""not fraud"" and ""unk"" (or some other label to indicate they are not in the class)--in other words, ""inside the decision boundary"" and ""outside the decision boundary."" 

I wanted to conclude by mentioning that, 20 years after their ""discovery"", the SVM is a firmly entrenched member in the ML library. And indeed, the consistently superior resolution compared with other state-of-the-art classifiers is well documented. 
Their pedigree is both a function of their superior performance documented in numerous rigorously controlled studies as well as their conceptual elegance. W/r/t the latter point, consider that multi-layer perceptrons (MLP), though they are often excellent classifiers, are driven by a numerical optimization routine, which in practice rarely finds the global minimum; moreover, that solution has no conceptual significance. On the other hand, the numerical optimization at the heart of building an SVM classifier does in fact find the global minimum. What's more that solution is the actual decision boundary.
Still, i think SVM reputation has declined a little during the past few years. 
The primary reason i suspect is the NetFlix competition. NetFlix emphasized the resolving power of fundamental techniques of matrix decomposition and even more significantly t*he power of combining classifiers. People combined classifiers long before NetFlix, but more as a contingent technique than as an attribute of classifier design. Moreover, many of the techniques for combining classifiers are extraordinarily simple to understand and also to implement. By contrast, SVMs are not only very difficult to code (in my opinion, by far the most difficult ML algorithm to implement in code) but also difficult to configure and implement as a pre-compiled library--e.g., a kernel must be selected, the results are very sensitive to how the data is re-scaled/normalized, etc.
"
semi-supervised learning,How I can split a fully labeled dataset into labeled and unlabeled data for semi-supervised learning purpose,"It is not going to give you an improved accuracy when using less labels. If you split you data in order to delete the labels from one group and use that with a semi-supervised learning, it will just reduce your accuracy.
The purpose of semi-supervised is that the process of labeling massive amounts of data for supervised learning is extremely time-consuming and expensive, so if you need more data (that you already have), then you can use techniques to use unlabeled data.
Before even thinking about the coding in Java, can you develop a bit more your idea of why you were thinking of this?
"
semi-supervised learning,Is there some way can accomplish stochastic gradient descent not from scratch,"Theano and Tensorflow will exactly do this for you if you can formulate your optimization-problem within their framework / language. These frameworks are also general enough to implement non-NN-based algorithms, like simple first-order-based optimizations like yours.
If that's not possible you can try autograd, which can do this on a subset of numpy. Just formulate your loss as numpy-function (while sticking to supported functions; read the docs) and let autograd build the gradients.
Keep in mind, that the somewhat by-construction-approach used by Theano & Tensorflow will be more efficient (because of the more defined input and because these two libraries are a bit more evolved).
"
semi-supervised learning,What is &quot;semantic segmentation&quot; compared to &quot;segmentation&quot; and &quot;scene labeling&quot;?,"""segmentation"" is a partition of an image into several ""coherent"" parts, but without any attempt at understanding what these parts represent. One of the most famous works (but definitely not the first) is Shi and Malik ""Normalized Cuts and Image Segmentation"" PAMI 2000. These works attempt to define ""coherence"" in terms of low-level cues such as color, texture and smoothness of boundary. You can trace back these works to the Gestalt theory.
On the other hand ""semantic segmentation"" attempts to partition the image into semantically meaningful parts, and to classify each part into one of the pre-determined classes. You can also achieve the same goal by classifying each pixel (rather than the entire image/segment). In that case you are doing pixel-wise classification, which leads to the same end result but in a slightly different path...
So, I suppose you can say that ""semantic segmentation"", ""scene labeling"" and ""pixelwise classification"" are basically trying to achieve the same goal: semantically understanding the role of each pixel in the image. You can take many paths to reach that goal, and these paths lead to slight nuances in the terminology. 
"
semi-supervised learning,Tensorflow different dropout configuration for one gradient update,"You can use tf.split() to split the batch into two halves, feed each half through a tf.layers.dropout() separately, and then re-concatenate them with tf.concat(). Something like this (pseudo-code):
splitted = tf.split( batch, 2, axis = 0 )
first_half = tf.dropout( splitted[ 0 ], rate = 0.5 )
second_half = tf.dropout( splitted[ 1 ], rate = 0.6 ) 
rejoined = tf.concatenate( [ first_half, second_half ], axis = 0 )

"
semi-supervised learning,How to interpret the loss function in the categorical generative adversarial net?,"I've done some research and asked some questions to my friends who works in a big company doing research on deep learning. As it turns out that the generative adversarial networks is not good at classification jobs. So I changed my mind and implemented it with GoogLenet. Problem solved!
"
semi-supervised learning,How can I learn *practical* natural language processing?,"NLTK is the way to go for you. :)
Also, if you are interested in implementing algorithms like LDA, LSA, etc. I would recommend to go with gensims
"
semi-supervised learning,Semi-supervised learning for regression by scikit-learn,"It looks like the error in the documentation, code itself clearly is classification only (beggining of the .fit call of the BasePropagation class):
    check_classification_targets(y)

    # actual graph construction (implementations should override this)
    graph_matrix = self._build_graph()

    # label construction
    # construct a categorical distribution for classification only
    classes = np.unique(y)
    classes = (classes[classes != -1])

In theory you could remove the ""check_classification_targets"" call and use ""regression like method"", but it will not be the true regression since you will never ""propagate"" any value which is not encountered in the training set, you will simply treat the regression value as the class identifier. And you will be unable to use value ""-1"" since it is a codename for ""unlabeled""...
"
semi-supervised learning,semi-supervised learning&#39;s testing data,"You train your classifier on L. You can firstly perform cross-validation to fit some method parameters P. With parameters P you construct model M, from labeled data L. You then use the model M to label unlabeled data U. You join the examples from U (with heighest confidence in assigned class) with L. You then repeat the procedure until all the examples are classiied.
-edit-
I think the most appropriate approach is the third one. But I may not understand it right, so here goes. 
You split L to L_train and L_test. You train your classifier using L_train and you also use this classifier to classify U (as per methodology I described above). From union of labeled U and L_train you construct a new classifier, and with it you classify L_test. The differences in these classification can be used for evaluation measures (classification accuracy, ...).
"
semi-supervised learning,What&#39;s the difference between collective classification and semi-supervised learning,"Semi supervised learning  is more general - it does not specify/stipulate the structure of the input data.  It can be summarized as ""learning from a combination of  labeled and unlabeled data points"".  The approach to performing the inference is also unspecified.
The ""Collective classification"" as you have reflected above does specify the way in which the unlabeled points are inferred: 

based on the classes assigned to the known nodes and the network
  structure only.

So there is an additional expectation on the data that they are
- represented in a graph structure
- their correlation can be used to computer their relative similarity and hence their class
A summary of Collective Classification from this paper https://www.cs.uic.edu/~xkong/sdm11_icml.pdf helps to illustrate the (higher) expectations on the data structure and semantics:

Collective classification in relational data has become animportant and
  active research topic in the last decade,where class labels for a
  group of linked instances are cor-related and need to be predicted
  simultaneously. 

The note about the types of problems applicable is also revealing - notice they are graph oriented data analysis tasks:

Collective classification has a wide variety of real
  world appli-cations,e.g.hyperlinked document classification,
  socialnetworks analysis and collaboration networks analysis

"
semi-supervised learning,Calculating minimum s-t cuts is not implemented yet in igraph,"You should be able to instead get the min cut with the stMincuts function from the igraph package:
library(igraph)
set.seed(144)
g <- erdos.renyi.game(10, .5, directed=TRUE)
cut <- stMincuts(g, source=1, target=4)

Now you can access the value:
cut$value
# [1] 4

the edges that were cut:
E(g)[cut$cuts[[1]]]
# Edge sequence:
#            
# [8]  1 -> 3
# [15] 1 -> 4
# [24] 1 -> 6
# [30] 1 -> 7

and the vertices in one partition:
V(g)[cut$partition1s[[1]]]
# Vertex sequence:
# [1] 1

If there are multiple cuts (there were two in the example I've provided here), you can get the edges/vertices with, for instance, cut$cuts[[2]] and cut$partition1s[[2]], ...
"
semi-supervised learning,Using ELKI MiniGUI for anomaly detection with training set and test set,"Your scenario is a supervised learning approach.
ELKI currently only includes unsupervised outlier detection methods, that do not make use of the prior information of ""normal only"" training data.
You could concatenate training and test files into one file, and then run outlier detection. Most published algorithms in this domain are unsupervised. In unsupervised learning, there is no training data set - there is only one kind of data.
Note that most algorithms available in ELKI as of 2014 are designed for numerical data. If your data is categorial, you will be able to use many of them, but you will need to implement data types and distance functions that fit your data type. There are some parsers and distances for non-numerical data available (e.g. for textual data) but this is not supported by the ARFF parser, and there is currently no distance function for mixed data either.
"
semi-supervised learning,Weka - semi supervised learning - how to label data and get back the result?,"My understanding is that you would like to store the predicted labels of your model into your missing labels.
What you could do is right-click on the Model after training, then select 'Visualize Classifier Errors'.    In this visualization screen, set Y as the predicted class and then save the new ARFF.  This datafile should then contain the predicted and class labels.
From there, you could try to replace the Missing Values with the predicted labels.
I hope this assists in the problem that you are experiencing.
"
semi-supervised learning,Weak learner in scikit learn random forest and extra tree classifiers,"You are correct, we are only doing axis-aligned splits. This is in fact the most common splitting strategy, giving in most of the cases good-enough results while not increasing computational complexity. 
"
semi-supervised learning,Weka ignoring unlabeled data,"The problem is that when you specify a training set -t train.arff and a test set test.arff, the mode of operation is to calculate the performance of the model based on the test set. But you can't calculate a performance of any kind without knowing the actual class. Without the actual class, how will you know if your prediction if right or wrong?
I used the data you gave as train.arff and as test.arff with arbitrary class labels assigned by me. The relevant output lines are:
=== Error on training data ===

Correctly Classified Instances           4               80      %
Incorrectly Classified Instances         1               20      %
Kappa statistic                          0.6154
Mean absolute error                      0.2429
Root mean squared error                  0.4016
Relative absolute error                 50.0043 %
Root relative squared error             81.8358 %
Total Number of Instances                5     


=== Confusion Matrix ===

 a b   <-- classified as
 2 1 | a = 1
 0 2 | b = -1

and
=== Error on test data ===

Total Number of Instances                0     
Ignored Class Unknown Instances                  5     


=== Confusion Matrix ===

 a b   <-- classified as
 0 0 | a = 1
 0 0 | b = -1

Weka can give you those statistics for the training set, because it knows the actual class labels and the predicted ones (applying the model on the training set). For the test set, it can't get any information about the performance, because it doesn't know about the true class labels.
What you might want to do is:
java -cp weka.jar weka.classifiers.bayes.NaiveBayes -t train.arff -T test.arff -p 1-4

which in my case would give you:
=== Predictions on test data ===

 inst#     actual  predicted error prediction (feature1,feature2,feature3,feature4)
     1        1:?        1:1       1 (1,7,1,0)
     2        1:?        1:1       1 (1,5,1,0)
     3        1:?       2:-1       0.786 (-1,1,1,0)
     4        1:?       2:-1       0.861 (1,1,1,1)
     5        1:?       2:-1       0.861 (-1,1,1,1)

So, you can get the predictions, but you can't get a performance, because you have unlabeled test data.
"
